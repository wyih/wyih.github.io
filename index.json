[{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Yihong WANG","type":"authors"},{"authors":["Yihong Wang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"ff79cbfdb6f25c145b01a9c01f7ab994","permalink":"/authors/wyih/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wyih/","section":"authors","summary":"","tags":null,"title":"Yihong WANG","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":["Books"],"content":" 原来The Catcher in Rye并不是讲稻草人和乌鸦的故事，是中二少年失败的离家出走尝试。如果我中二期看的这书，应该会很喜欢吧。虽然现在也挺喜欢的。更奇妙的是这么多年了，竟然一点没被剧透。To Kill A Mockingbird也是如此，并不是一个讲猎人的故事，我到底是有多文盲啊！\n","date":1573948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573950108,"objectID":"e2a0760907a3843f842e014066134fac","permalink":"/post/2019-11-17-catcher-in-rye/","publishdate":"2019-11-17T00:00:00Z","relpermalink":"/post/2019-11-17-catcher-in-rye/","section":"post","summary":"原来The Catcher in Rye并不是讲稻草人和乌鸦的故事，是中二少年失败的离家出走尝试。如果我中二期看的这书，应该会很喜欢吧。虽然现在也挺喜欢的。更奇","tags":["Reading","Books"],"title":"The Catcher in Rye","type":"post"},{"authors":[],"categories":["Linux"],"content":"  缘起 折腾备忘录 安好之后换中国源 N卡驱动 中文输入法 Deepin桌面 安装miniconda 搭建服务并打开端口 Chrome Remote Desktop 系统备份和恢复    缘起 一切的开始应该是从折腾家庭影院开始。最早的解决方案是Windows做服务器，不太理想，于是入手了黑群晖。在黑群晖一路走来，点亮了无数新的技能点。再加上非常幸运的有公网IP，可以折腾的余地大大增加了。在群晖系统里玩了一阵子docker之后，就想着要搞一个Linux来玩玩，不想用自己的台式机折腾，查了查说最好的linux笔记本是Chromebook，说是丝滑般的Chrome体验以及是续航最久的Linux本子，加上便宜，果断入手。到后之后发现真不错，Chrome OS再加上Android再加上Linux简直了，基本上出门的需求可以满足，虽然说我也不爱出门。但玩着玩着看到人家说最好的Linux发行版是WSL，WSL需要更新windows 10，但我家里的台式机一直停留在15年的windows版本，一升级就蓝屏循环中，这回干脆咬牙升级了下系统，玩上了WSL，想用来开docker布服务吧，我看着Windows的防火墙就头疼，还是算了。但在用Chromebook的过程中发现这个触摸板手势真的很爽啊！想在Windows下也有这么爽，入了一个联想触摸板，是旧型号，只支持Windows 8的手势，突然又幻想Linux下对触摸板的驱动是不是好些呢（做梦，最后实践表明Manjaro根本只把它认成鼠标而非触摸板），于是搞起了双系统…\n现在的结果是爽死了，感觉自己省了好多买服务器的钱！我真是太机智了！\n 折腾备忘录 以防将来又需要重装，写下安装的注意事项供未来的我参考。安装iso用的是Manjaro KDE，不要用xfce版。\n安好之后换中国源 # 中国区镜像排序，一般选择前两个镜像 sudo pacman-mirrors -i -c China -m rank ##更新数据源 sudo pacman -Syy ## 添加archlinuxcn源 sudo nano /etc/pacman.conf 在文件最后添加\n[archlinuxcn] SigLevel = Optional TrustedOnly Server = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/$arch sudo pacman -Syyu //更新数据源 sudo pacman -S archlinuxcn-keyring //安装导入GPG key  N卡驱动 sudo mhwd -a pci nonfree 0300 sudo reboot nvidia-settings  中文输入法 #中文字体 sudo pacman -S adobe-source-han-sans-cn-fonts adobe-source-han-serif-cn-fonts sudo pacman -S fcitx fcitx-googlepinyin fcitx-im fcitx-configtool # 编辑 ~/.xinitrc sudo nano ~/.xprofile export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=\u0026quot;@im=fcitx\u0026quot;  Deepin桌面 安装dde\nsudo pacman -S deepin deepin-extra 修改 /etc/lightdm/lightdm.conf\nsudo cp /etc/lightdm/lightdm.conf /etc/lightdm/lightdm.conf.bak sudo sed -i \u0026#39;s/greeter-session=lightdm-.*/greeter-session=lightdm-deepin-greeter/g\u0026#39; /etc/lightdm/lightdm.conf sudo sed -i \u0026#39;s/user-session=xfce/user-session=deepin/g\u0026#39; /etc/lightdm/lightdm.conf 选择桌面:注销账户，在登录界面右下角选择 deepin 桌面图标\n 安装miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh sh Miniconda3-latest-Linux-x86_64.sh # 编辑 ~/.bash_profile,在最后添加如下环境变量（注意PATH要在前面） export PATH=\u0026quot;$PATH:$HOME/miniconda3/bin\u0026quot; # 编辑完成后 source .bash_profile # 进入base环境或新建的python环境 source activate pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple 之后便可conda和pip安装包了。\n 搭建服务并打开端口 用的是ufw。\nRstudio Server开机自动运行 sudo rstudio-server verify-installation # 查看狀態 systemctl status rstudio-server # 啟動 systemctl start rstudio-server # 關閉 systemctl stop rstudio-server #auto start sudo systemctl enable rstudio-server 太爽了！这篇post就是在Rstudio Server写就。\n Jupyter lab 在/etc/systemd/system下添加jupyter.service文件\n#sudo nano /etc/systemd/system/jupyter.service [Unit] Description=Jupyter Lab [Service] Type=simple PIDFile=/run/jupyter.pid ExecStart=/home/wyih/anaconda3/bin/jupyter lab --ip 192.168.6.100 --config=/home/wyih/.jupyter/jupyter_notebook_config.py User=wyih Group=wyih WorkingDirectory=/home/wyih/Jupyter Notebook Restart=always RestartSec=10 [Install] WantedBy=multi-user.target 开启服务\nsystemctl enable jupyter.service systemctl daemon-reload systemctl restart jupyter.service jupyter_notebook_config.py配置:\nc.NotebookApp.ip = \u0026#39;*\u0026#39; # 允许访问此服务器的 IP，星号表示任意 IP c.NotebookApp.password = u\u0026#39;sha1:xxx:xxx\u0026#39; # 之前生成的密码 hash 字串 c.NotebookApp.open_browser = False # 运行时不打开本机浏览器 c.NotebookApp.port = 8889 # 使用的端口 c.NotebookApp.allow_remote_access = True ## 是否允许notebook在root用户下运行. c.NotebookApp.allow_root = True   Chrome Remote Desktop installed “chrome-remote-desktop” from AUR and Chrome extension. Executed crd --setup in the terminal as normal user - was requested sudo password edited “.chrome-remote-desktop-session” file deleting the # in front of “exec /usr/bin/startkde” line accepted screen resolution executed crd --restart  好像还是不能开始自动运行CRD。\n 系统备份和恢复 还没研究明白。\n  ","date":1572998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572999708,"objectID":"1e98cae8cc52fac58958157e9b87d5b2","permalink":"/post/manjaro/","publishdate":"2019-11-06T00:00:00Z","relpermalink":"/post/manjaro/","section":"post","summary":"缘起 折腾备忘录 安好之后换中国源 N卡驱动 中文输入法 Deepin桌面 安装miniconda 搭建服务并打开端口 Chrome Remote Desktop 系统备份和恢复 缘起 一切的开始应","tags":["Linux"],"title":"Manjaro折腾记","type":"post"},{"authors":[],"categories":["R"],"content":"  Use Color Palette Layer Color and Text Together Themes Use Theme Elements Two y-axes   head(asasec) ## Section Sname Beginning Revenues ## 1 Aging and the Life Course (018) Aging 12752 12104 ## 2 Alcohol, Drugs and Tobacco (030) Alcohol/Drugs 11933 1144 ## 3 Altruism and Social Solidarity (047) Altruism 1139 1862 ## 4 Animals and Society (042) Animals 473 820 ## 5 Asia/Asian America (024) Asia 9056 2116 ## 6 Body and Embodiment (048) Body 3408 1618 ## Expenses Ending Journal Year Members ## 1 12007 12849 No 2005 598 ## 2 400 12677 No 2005 301 ## 3 1875 1126 No 2005 NA ## 4 1116 177 No 2005 209 ## 5 1710 9462 No 2005 365 ## 6 1920 3106 No 2005 NA p \u0026lt;- ggplot( data = subset(asasec, Year == 2014), mapping = aes(x = Members, y = Revenues, label = Sname) ) p + geom_point() + geom_smooth() p \u0026lt;- ggplot( data = subset(asasec, Year == 2014), mapping = aes(x = Members, y = Revenues, label = Sname) ) p + geom_point(mapping = aes(color = Journal)) + geom_smooth(method = \u0026quot;lm\u0026quot;) p0 \u0026lt;- ggplot( data = subset(asasec, Year == 2014), mapping = aes(x = Members, y = Revenues, label = Sname) ) p1 \u0026lt;- p0 + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE, color = \u0026quot;gray80\u0026quot;) + geom_point(mapping = aes(color = Journal)) library(ggrepel) p2 \u0026lt;- p1 + geom_text_repel(data = subset(asasec, Year == 2014 \u0026amp; Revenues \u0026gt; 7000), size = 2) p3 \u0026lt;- p2 + labs( x = \u0026quot;Membership\u0026quot;, y = \u0026quot;Revenues\u0026quot;, color = \u0026quot;Section has own Journal\u0026quot;, title = \u0026quot;ASA Sections\u0026quot;, subtitle = \u0026quot;2014 Calendar year.\u0026quot;, caption = \u0026quot;Source: ASA annual report.\u0026quot; ) p4 \u0026lt;- p3 + scale_y_continuous(labels = scales::dollar) + theme(legend.position = \u0026quot;bottom\u0026quot;) p4 Use Color Palette Use the RColorBrewer package. Access the colors by specifying the scale_color_brewer() or scale_ﬁll_brewer() functions, depending on the aesthetic you are mapping.\np \u0026lt;- ggplot(data = organdata, mapping = aes(x = roads, y = donors, color = world)) p + geom_point(size = 2) + scale_color_brewer(palette = \u0026quot;Set2\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) p + geom_point(size = 2) + scale_color_brewer(palette = \u0026quot;Pastel2\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) p + geom_point(size = 2) + scale_color_brewer(palette = \u0026quot;Dark2\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) Specify colors manually, via scale_color_manual() or scale_fill_manual(). Try demo('color') to see the color names in R.\ncb_palette \u0026lt;- c( \u0026quot;#999999\u0026quot;, \u0026quot;#E69F00\u0026quot;, \u0026quot;#56B4E9\u0026quot;, \u0026quot;#009E73\u0026quot;, \u0026quot;#F0E442\u0026quot;, \u0026quot;#0072B2\u0026quot;, \u0026quot;#D55E00\u0026quot;, \u0026quot;#CC79A7\u0026quot; ) p4 + scale_color_manual(values = cb_palette) library(dichromat) library(RColorBrewer) Default \u0026lt;- brewer.pal(5, \u0026quot;Set2\u0026quot;) types \u0026lt;- c(\u0026quot;deutan\u0026quot;, \u0026quot;protan\u0026quot;, \u0026quot;tritan\u0026quot;) names(types) \u0026lt;- c(\u0026quot;Deuteronopia\u0026quot;, \u0026quot;Protanopia\u0026quot;, \u0026quot;Tritanopia\u0026quot;) color_table \u0026lt;- types %\u0026gt;% purrr::map(~ dichromat(Default, .x)) %\u0026gt;% as_tibble() %\u0026gt;% add_column(Default, .before = TRUE) color_table ## # A tibble: 5 x 4 ## Default Deuteronopia Protanopia Tritanopia ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 #66C2A5 #AEAEA7 #BABAA5 #82BDBD ## 2 #FC8D62 #B6B661 #9E9E63 #F29494 ## 3 #8DA0CB #9C9CCB #9E9ECB #92ABAB ## 4 #E78AC3 #ACACC1 #9898C3 #DA9C9C ## 5 #A6D854 #CACA5E #D3D355 #B6C8C8  Layer Color and Text Together # Democrat Blue and Republican Red party_colors ← c(\u0026quot;#2E74C0\u0026quot;, \u0026quot;#CB454A\u0026quot;) p0 \u0026lt;- ggplot( data = subset(county_data, flipped == \u0026quot;No\u0026quot;), mapping = aes(x = pop, y = black / 100) ) p1 \u0026lt;- p0 + geom_point(alpha = 0.15, color = \u0026quot;gray50\u0026quot;) + scale_x_log10(labels = scales::comma) p1 party_colors \u0026lt;- c(\u0026quot;#2E74C0\u0026quot;, \u0026quot;#CB454A\u0026quot;) p2 \u0026lt;- p1 + geom_point( data = subset(county_data, flipped == \u0026quot;Yes\u0026quot;), mapping = aes(x = pop, y = black / 100, color = partywinner16) ) + scale_color_manual(values = party_colors) p2 p3 \u0026lt;- p2 + scale_y_continuous(labels = scales::percent) + labs( color = \u0026quot;County flipped to ... \u0026quot;, x = \u0026quot;County Population (log scale)\u0026quot;, y = \u0026quot;Percent Black Population\u0026quot;, title = \u0026quot;Flipped counties, 2016\u0026quot;, caption = \u0026quot;Counties in gray did not flip.\u0026quot; ) p3 p4 \u0026lt;- p3 + geom_text_repel( data = subset(county_data, flipped == \u0026quot;Yes\u0026quot; \u0026amp; black \u0026gt; 25), mapping = aes(x = pop, y = black / 100, label = state), size = 2 ) p4 + theme_minimal() + theme(legend.position = \u0026quot;top\u0026quot;)  Themes theme_set(theme_bw()) p4 + theme(legend.position = \u0026quot;top\u0026quot;) theme_set(theme_dark()) p4 + theme(legend.position = \u0026quot;top\u0026quot;) p4 + theme_gray() library(ggthemes) theme_set(theme_economist()) p4 + theme(legend.position = \u0026quot;top\u0026quot;) theme_set(theme_wsj()) p4 + theme( plot.title = element_text(size = rel(0.6)), legend.title = element_text(size = rel(0.35)), plot.caption = element_text(size = rel(0.35)), legend.position = \u0026quot;top\u0026quot; ) Claus O. Wilke’s cowplot package, contains a well-developed theme suitable for figures whose final destination is a journal article. BobRudis’s hrbrthemes package, has a distinctive and compact look and feel that takes advantage of some freely available typefaces.\nlibrary(hrbrthemes) theme_set(theme_ipsum()) p4 + theme(legend.position = \u0026quot;top\u0026quot;) p4 + theme( legend.position = \u0026quot;top\u0026quot;, plot.title = element_text( size = rel(2), lineheight = .5, family = \u0026quot;Times\u0026quot;, face = \u0026quot;bold.italic\u0026quot;, colour = \u0026quot;orange\u0026quot; ), axis.text.x = element_text( size = rel(1.1), family = \u0026quot;Courier\u0026quot;, face = \u0026quot;bold\u0026quot;, color = \u0026quot;purple\u0026quot; ) )  Use Theme Elements yrs \u0026lt;- c(seq(1972, 1988, 4), 1993, seq(1996, 2016, 4)) mean_age \u0026lt;- gss_lon %\u0026gt;% filter(age %nin% NA \u0026amp;\u0026amp; year %in% yrs) %\u0026gt;% group_by(year) %\u0026gt;% summarize(xbar = round(mean(age, na.rm = TRUE), 0)) mean_age$y \u0026lt;- 0.3 yr_labs \u0026lt;- data.frame(x = 85, y = 0.8, year = yrs) p \u0026lt;- ggplot(data = subset(gss_lon, year %in% yrs), mapping = aes(x = age)) p1 \u0026lt;- p + geom_density( fill = \u0026quot;gray20\u0026quot;, color = FALSE, alpha = 0.9, mapping = aes(y = ..scaled..) ) + geom_vline( data = subset(mean_age, year %in% yrs), aes(xintercept = xbar), color = \u0026quot;white\u0026quot;, size = 0.5 ) + geom_text( data = subset(mean_age, year %in% yrs), aes(x = xbar, y = y, label = xbar), nudge_x = 7.5, color = \u0026quot;white\u0026quot;, size = 3.5, hjust = 1 ) + geom_text(data = subset(yr_labs, year %in% yrs), aes(x = x, y = y, label = year)) + facet_grid(year ~ ., switch = \u0026quot;y\u0026quot;) p1 + theme( plot.title = element_text(size = 16), axis.text.x = element_text(size = 12), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), strip.background = element_blank(), strip.text.y = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank() ) + labs(x = \u0026quot;Age\u0026quot;, y = NULL, title = \u0026quot;Age Distribution of\\nGSS Respondents\u0026quot;) library(ggridges) p \u0026lt;- ggplot(data = gss_lon, mapping = aes(x = age, y = factor( year, levels = rev(unique(year)), ordered = TRUE ))) p + geom_density_ridges(alpha = 0.6, fill = \u0026quot;lightblue\u0026quot;, scale = 1.5) + scale_x_continuous(breaks = c(25, 50, 75)) + scale_y_discrete(expand = c(0.01, 0)) + labs(x = \u0026quot;Age\u0026quot;, y = NULL, title = \u0026quot;Age Distribution of\\nGSS Respondents\u0026quot;) + theme_ridges() + theme(title = element_text(size = 16, face = \u0026quot;bold\u0026quot;))  Two y-axes head(fredts) ## date sp500 monbase sp500_i monbase_i ## 1 2009-03-11 696.68 1542228 100.0000 100.0000 ## 2 2009-03-18 766.73 1693133 110.0548 109.7849 ## 3 2009-03-25 799.10 1693133 114.7012 109.7849 ## 4 2009-04-01 809.06 1733017 116.1308 112.3710 ## 5 2009-04-08 830.61 1733017 119.2240 112.3710 ## 6 2009-04-15 852.21 1789878 122.3245 116.0579 fredts_m \u0026lt;- fredts %\u0026gt;% select(date, sp500_i, monbase_i) %\u0026gt;% gather(key = series, value = score, sp500_i:monbase_i) head(fredts_m) ## date series score ## 1 2009-03-11 sp500_i 100.0000 ## 2 2009-03-18 sp500_i 110.0548 ## 3 2009-03-25 sp500_i 114.7012 ## 4 2009-04-01 sp500_i 116.1308 ## 5 2009-04-08 sp500_i 119.2240 ## 6 2009-04-15 sp500_i 122.3245 p \u0026lt;- ggplot(data = fredts_m, mapping = aes( x = date, y = score, group = series, color = series )) p1 \u0026lt;- p + geom_line() + theme(legend.position = \u0026quot;top\u0026quot;) + labs(x = \u0026quot;Date\u0026quot;, y = \u0026quot;Index\u0026quot;, color = \u0026quot;Series\u0026quot;) p \u0026lt;- ggplot(data = fredts, mapping = aes(x = date, y = sp500_i - monbase_i)) p2 \u0026lt;- p + geom_line() + labs(x = \u0026quot;Date\u0026quot;, y = \u0026quot;Difference\u0026quot;) cowplot::plot_grid(p1, p2, nrow = 2, rel_heights = c(0.75, 0.25), align = \u0026quot;v\u0026quot;) Using two y-axes gives you an extra degree of freedom to mess about with the data that, in most cases, you really should not take advantage of.\np \u0026lt;- ggplot(data = yahoo, mapping = aes(x = Employees, y = Revenue)) p + geom_path(color = \u0026quot;gray80\u0026quot;) + geom_text(aes(color = Mayer, label = Year), size = 3, fontface = \u0026quot;bold\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs( color = \u0026quot;Mayer is CEO\u0026quot;, x = \u0026quot;Employees\u0026quot;, y = \u0026quot;Revenue (Millions)\u0026quot;, title = \u0026quot;Yahoo Employees vs Revenues, 2004-2014\u0026quot; ) + scale_y_continuous(labels = scales::dollar) + scale_x_continuous(labels = scales::comma) p \u0026lt;- ggplot(data = yahoo, mapping = aes(x = Year, y = Revenue / Employees)) p + geom_vline(xintercept = 2012) + geom_line(color = \u0026quot;gray60\u0026quot;, size = 2) + annotate( \u0026quot;text\u0026quot;, x = 2013, y = 0.44, label = \u0026quot; Mayer becomes CEO\u0026quot;, size = 2.5 ) + labs(x = \u0026quot;Year\\n\u0026quot;, y = \u0026quot;Revenue/Employees\u0026quot;, title = \u0026quot;Yahoo Revenue to Employee Ratio, 2004-2014\u0026quot;) Saying no to pie\np_xlab \u0026lt;- \u0026quot;Amount Owed, in thousands of Dollars\u0026quot; p_title \u0026lt;- \u0026quot;Outstanding Student Loans\u0026quot; p_subtitle \u0026lt;- \u0026quot;44 million borrowers owe a total of $1.3 trillion\u0026quot; p_caption \u0026lt;- \u0026quot;Source: FRB NY\u0026quot; f_labs \u0026lt;- c(`Borrowers` = \u0026quot;Percent of\\nall Borrowers\u0026quot;, `Balances` = \u0026quot;Percent of\\nall Balances\u0026quot;) p \u0026lt;- ggplot(data = studebt, mapping = aes(x = Debt, y = pct / 100, fill = type)) p + geom_bar(stat = \u0026quot;identity\u0026quot;) + scale_fill_brewer(type = \u0026quot;qual\u0026quot;, palette = \u0026quot;Dark2\u0026quot;) + scale_y_continuous(labels = scales::percent) + guides(fill = FALSE) + theme(strip.text.x = element_text(face = \u0026quot;bold\u0026quot;)) + labs( y = NULL, x = p_xlab, caption = p_caption, title = p_title, subtitle = p_subtitle ) + facet_grid( ~ type, labeller = as_labeller(f_labs)) + coord_flip() library(viridis) p \u0026lt;- ggplot(studebt, aes(y = pct / 100, x = type, fill = Debtrc)) p + geom_bar(stat = \u0026quot;identity\u0026quot;, color = \u0026quot;gray80\u0026quot;) + scale_x_discrete(labels = as_labeller(f_labs)) + scale_y_continuous(labels = scales::percent) + scale_fill_viridis(discrete = TRUE) + guides( fill = guide_legend( reverse = TRUE, title.position = \u0026quot;top\u0026quot;, label.position = \u0026quot;bottom\u0026quot;, keywidth = 3, nrow = 1 ) ) + labs( x = NULL, y = NULL, fill = \u0026quot;Amount Owed, in thousands of dollars\u0026quot;, caption = p_caption, title = p_title, subtitle = p_subtitle ) + theme( legend.position = \u0026quot;top\u0026quot;, axis.text.y = element_text(face = \u0026quot;bold\u0026quot;, hjust = 1, size = 12), axis.ticks.length = unit(0, \u0026quot;cm\u0026quot;), panel.grid.major.y = element_blank() ) + coord_flip() http://r-graph-gallery.com/ for more examples\n ","date":1570579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570580508,"objectID":"6a4fb6e029cec15f2a83f04eec74b4d0","permalink":"/post/data-vis-chapter-8/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/post/data-vis-chapter-8/","section":"post","summary":"Use Color Palette Layer Color and Text Together Themes Use Theme Elements Two y-axes   head(asasec) ## Section Sname Beginning Revenues ## 1 Aging and the Life Course (018) Aging 12752 12104 ## 2 Alcohol, Drugs and Tobacco (030) Alcohol/Drugs 11933 1144 ## 3 Altruism and Social Solidarity (047) Altruism 1139 1862 ## 4 Animals and Society (042) Animals 473 820 ## 5 Asia/Asian America (024) Asia 9056 2116 ## 6 Body and Embodiment (048) Body 3408 1618 ## Expenses Ending Journal Year Members ## 1 12007 12849 No 2005 598 ## 2 400 12677 No 2005 301 ## 3 1875 1126 No 2005 NA ## 4 1116 177 No 2005 209 ## 5 1710 9462 No 2005 365 ## 6 1920 3106 No 2005 NA p \u0026lt;- ggplot( data = subset(asasec, Year == 2014), mapping = aes(x = Members, y = Revenues, label = Sname) ) p + geom_point() + geom_smooth() p \u0026lt;- ggplot( data = subset(asasec, Year == 2014), mapping = aes(x = Members, y = Revenues, label = Sname) ) p + geom_point(mapping = aes(color = Journal)) + geom_smooth(method = \u0026quot;lm\u0026quot;) p0 \u0026lt;- ggplot( data = subset(asasec, Year == 2014), mapping = aes(x = Members, y = Revenues, label = Sname) ) p1 \u0026lt;- p0 + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE, color = \u0026quot;gray80\u0026quot;) + geom_point(mapping = aes(color = Journal)) library(ggrepel) p2 \u0026lt;- p1 + geom_text_repel(data = subset(asasec, Year == 2014 \u0026amp; Revenues \u0026gt; 7000), size = 2) p3 \u0026lt;- p2 + labs( x = \u0026quot;Membership\u0026quot;, y = \u0026quot;Revenues\u0026quot;, color = \u0026quot;Section has own Journal\u0026quot;, title = \u0026quot;ASA Sections\u0026quot;, subtitle = \u0026quot;2014 Calendar year.","tags":["socviz"],"title":"Data Vis Chapter 8","type":"post"},{"authors":[],"categories":["R"],"content":"  Show Several Fits at Once, with a Legend Model-based Graphics Tidy Model Objects with Broom get component-level statistics with tidy() Get observation-level statistics with augment()  Grouped Analysis Plots for Surveys   p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) p + geom_point(alpha = 0.1) + geom_smooth(color = \u0026quot;tomato\u0026quot;, fill = \u0026quot;tomato\u0026quot;, method = MASS::rlm) + #robust regression line geom_smooth(color = \u0026quot;steelblue\u0026quot;, fill = \u0026quot;steelblue\u0026quot;, method = \u0026quot;lm\u0026quot;) p + geom_point(alpha = 0.1) + geom_smooth( color = \u0026quot;tomato\u0026quot;, method = \u0026quot;lm\u0026quot;, size = 1.2, formula = y ~ splines::bs(x, 3), se = FALSE ) p + geom_point(alpha = 0.1) + geom_quantile( # specialized version of geom)smooth that can fit quantile regression color = \u0026quot;tomato\u0026quot;, size = 1.2, method = \u0026quot;rqss\u0026quot;, lambda = 1, quantiles = c(0.20, 0.5, 0.85) ) ## Smoothing formula not specified. Using: y ~ qss(x, lambda = 1) Show Several Fits at Once, with a Legend model_colors \u0026lt;- RColorBrewer::brewer.pal(3, \u0026quot;Set1\u0026quot;) model_colors ## [1] \u0026quot;#E41A1C\u0026quot; \u0026quot;#377EB8\u0026quot; \u0026quot;#4DAF4A\u0026quot; p0 \u0026lt;- ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) p1 \u0026lt;- p0 + geom_point(alpha = 0.2) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = \u0026quot;OLS\u0026quot;, fill = \u0026quot;OLS\u0026quot;)) + geom_smooth( method = \u0026quot;lm\u0026quot;, formula = y ~ splines::bs(x, df = 3), aes(color = \u0026quot;Cubic Spline\u0026quot;, fill = \u0026quot;Cubic Spline\u0026quot;) ) + geom_smooth(method = \u0026quot;loess\u0026quot;, aes(color = \u0026quot;LOESS\u0026quot;, fill = \u0026quot;LOESS\u0026quot;)) p1 + scale_color_manual(name = \u0026quot;Models\u0026quot;, values = model_colors) + scale_fill_manual(name = \u0026quot;Models\u0026quot;, values = model_colors) + theme(legend.position = \u0026quot;top\u0026quot;)  Model-based Graphics min_gdp \u0026lt;- min(gapminder$gdpPercap) max_gdp \u0026lt;- max(gapminder$gdpPercap) med_pop \u0026lt;- median(gapminder$pop) pred_df \u0026lt;- expand.grid(gdpPercap = (seq(from = min_gdp, to = max_gdp, length.out = 100)), pop = med_pop, continent = c(\u0026quot;Africa\u0026quot;, \u0026quot;Americas\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Europe\u0026quot;, \u0026quot;Oceania\u0026quot;)) dim(pred_df) ## [1] 500 3 out \u0026lt;- lm(formula = lifeExp ~ gdpPercap + pop + continent, data = gapminder) pred_out \u0026lt;- predict(object = out, newdata = pred_df, interval = \u0026quot;predict\u0026quot;) pred_df \u0026lt;- cbind(pred_df, pred_out) p \u0026lt;- ggplot( data = subset(pred_df, continent %in% c(\u0026quot;Europe\u0026quot;, \u0026quot;Africa\u0026quot;)), aes( x = gdpPercap, y = fit, ymin = lwr, ymax = upr, color = continent, fill = continent, group = continent ) ) p + geom_point( data = subset(gapminder, continent %in% c(\u0026quot;Europe\u0026quot;, \u0026quot;Africa\u0026quot;)), aes(x = gdpPercap, y = lifeExp, color = continent), alpha = 0.5, inherit.aes = FALSE ) + geom_line() + geom_ribbon(alpha = 0.2, color = FALSE) + scale_x_log10(labels = scales::dollar)  Tidy Model Objects with Broom get component-level statistics with tidy() library(broom) out_comp \u0026lt;- tidy(out) out_comp %\u0026gt;% round_df() ## # A tibble: 7 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 47.8 0.34 141. 0 ## 2 gdpPercap 0 0 19.2 0 ## 3 pop 0 0 3.33 0 ## 4 continentAmericas 13.5 0.6 22.5 0 ## 5 continentAsia 8.19 0.570 14.3 0 ## 6 continentEurope 17.5 0.62 28.0 0 ## 7 continentOceania 18.1 1.78 10.2 0 “not in” %nin% is availabe via socviz. prefix_strip from socviz drops prefixes\n#confidence interval out_conf \u0026lt;- tidy(out, conf.int = TRUE) out_conf \u0026lt;- subset(out_conf, term %nin% \u0026quot;(Intercept)\u0026quot;) out_conf$nicelabs \u0026lt;- prefix_strip(out_conf$term, \u0026quot;continent\u0026quot;) p \u0026lt;- ggplot(out_conf, mapping = aes( x = reorder(nicelabs, estimate), y = estimate, ymin = conf.low, ymax = conf.high )) p + geom_pointrange() + coord_flip() + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;OLS Estimate\u0026quot;)  Get observation-level statistics with augment() out_aug \u0026lt;- augment(out) p \u0026lt;- ggplot(data = out_aug, mapping = aes(x = .fitted, y = .resid)) p + geom_point() ### Get model-level statistics with glance() Broom is able to tidy (and augment, and glance at) a wide range of model types.\nlibrary(survival) out_cph \u0026lt;- coxph(Surv(time, status) ~ age + sex, data = lung) out_surv \u0026lt;- survfit(out_cph) out_tidy \u0026lt;- tidy(out_surv) p \u0026lt;- ggplot(data = out_tidy, mapping = aes(time, estimate)) p + geom_line() + geom_ribbon(mapping = aes(ymin = conf.low, ymax = conf.high), alpha = 0.2)   Grouped Analysis nest and unnest\nout_le \u0026lt;- gapminder %\u0026gt;% group_by(continent, year) %\u0026gt;% nest() fit_ols \u0026lt;- function(df) { lm(lifeExp ~ log(gdpPercap), data = df) } out_le \u0026lt;- gapminder %\u0026gt;% group_by(continent, year) %\u0026gt;% nest() %\u0026gt;% mutate(model = map(data, fit_ols)) out_tidy \u0026lt;- gapminder %\u0026gt;% group_by(continent, year) %\u0026gt;% nest() %\u0026gt;% mutate(model = map(data, fit_ols), tidied = map(model, tidy)) %\u0026gt;% unnest(tidied, .drop = TRUE) %\u0026gt;% filter(term %nin% \u0026quot;(Intercept)\u0026quot; \u0026amp; continent %nin% \u0026quot;Oceania\u0026quot;) ## Warning: The `.drop` argument of `unnest()` is deprecated as of tidyr 1.0.0. ## All list-columns are now preserved. ## This warning is displayed once per session. ## Call `lifecycle::last_warnings()` to see where this warning was generated. p \u0026lt;- ggplot( data = out_tidy, mapping = aes( x = year, y = estimate, ymin = estimate - 2 * std.error, ymax = estimate + 2 * std.error, group = continent, color = continent ) ) p + geom_pointrange(position = position_dodge(width = 1)) + scale_x_continuous(breaks = unique(gapminder$year)) + theme(legend.position = \u0026quot;top\u0026quot;) + labs(x = \u0026quot;Year\u0026quot;, y = \u0026quot;Estimate\u0026quot;, color = \u0026quot;Continent\u0026quot;) ## Plot Marginal Effects\nlibrary(margins) gss_sm$polviews_m \u0026lt;- relevel(gss_sm$polviews, ref = \u0026quot;Moderate\u0026quot;) out_bo \u0026lt;- glm(obama ~ polviews_m + sex * race, family = \u0026quot;binomial\u0026quot;, data = gss_sm) summary(out_bo) ## ## Call: ## glm(formula = obama ~ polviews_m + sex * race, family = \u0026quot;binomial\u0026quot;, ## data = gss_sm) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9045 -0.5541 0.1772 0.5418 2.2437 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 0.296493 0.134091 2.211 0.02703 * ## polviews_mExtremely Liberal 2.372950 0.525045 4.520 6.20e-06 *** ## polviews_mLiberal 2.600031 0.356666 7.290 3.10e-13 *** ## polviews_mSlightly Liberal 1.293172 0.248435 5.205 1.94e-07 *** ## polviews_mSlightly Conservative -1.355277 0.181291 -7.476 7.68e-14 *** ## polviews_mConservative -2.347463 0.200384 -11.715 \u0026lt; 2e-16 *** ## polviews_mExtremely Conservative -2.727384 0.387210 -7.044 1.87e-12 *** ## sexFemale 0.254866 0.145370 1.753 0.07956 . ## raceBlack 3.849526 0.501319 7.679 1.61e-14 *** ## raceOther -0.002143 0.435763 -0.005 0.99608 ## sexFemale:raceBlack -0.197506 0.660066 -0.299 0.76477 ## sexFemale:raceOther 1.574829 0.587657 2.680 0.00737 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2247.9 on 1697 degrees of freedom ## Residual deviance: 1345.9 on 1686 degrees of freedom ## (1169 observations deleted due to missingness) ## AIC: 1369.9 ## ## Number of Fisher Scoring iterations: 6 bo_m \u0026lt;- margins(out_bo) summary(bo_m) ## factor AME SE z p lower ## polviews_mConservative -0.4119 0.0283 -14.5394 0.0000 -0.4674 ## polviews_mExtremely Conservative -0.4538 0.0420 -10.7971 0.0000 -0.5361 ## polviews_mExtremely Liberal 0.2681 0.0295 9.0996 0.0000 0.2103 ## polviews_mLiberal 0.2768 0.0229 12.0736 0.0000 0.2319 ## polviews_mSlightly Conservative -0.2658 0.0330 -8.0596 0.0000 -0.3304 ## polviews_mSlightly Liberal 0.1933 0.0303 6.3896 0.0000 0.1340 ## raceBlack 0.4032 0.0173 23.3568 0.0000 0.3694 ## raceOther 0.1247 0.0386 3.2297 0.0012 0.0490 ## sexFemale 0.0443 0.0177 2.5073 0.0122 0.0097 ## upper ## -0.3564 ## -0.3714 ## 0.3258 ## 0.3218 ## -0.2011 ## 0.2526 ## 0.4371 ## 0.2005 ## 0.0789 The margins library comes with several plot methods of its own. If you wish, at this point you can just try plot(bo_m) to see a plot of the average marginal effects, produced with the general look of a Stata graphic. Other plot methods in the margins library include cplot(), which visualizes marginal effects conditional on a second variable, and image(), which shows predictions or marginal effects as a filled heatmap or contour plot.\nbo_gg \u0026lt;- as_tibble(summary(bo_m)) prefixes \u0026lt;- c(\u0026quot;polviews_m\u0026quot;, \u0026quot;sex\u0026quot;) bo_gg$factor \u0026lt;- prefix_strip(bo_gg$factor, prefixes) bo_gg$factor \u0026lt;- prefix_replace(bo_gg$factor, \u0026quot;race\u0026quot;, \u0026quot;Race: \u0026quot;) bo_gg %\u0026gt;% select(factor, AME, lower, upper) ## # A tibble: 9 x 4 ## factor AME lower upper ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Conservative -0.412 -0.467 -0.356 ## 2 Extremely Conservative -0.454 -0.536 -0.371 ## 3 Extremely Liberal 0.268 0.210 0.326 ## 4 Liberal 0.277 0.232 0.322 ## 5 Slightly Conservative -0.266 -0.330 -0.201 ## 6 Slightly Liberal 0.193 0.134 0.253 ## 7 Race: Black 0.403 0.369 0.437 ## 8 Race: Other 0.125 0.0490 0.200 ## 9 Female 0.0443 0.00967 0.0789 p \u0026lt;- ggplot(data = bo_gg, aes( x = reorder(factor, AME), y = AME, ymin = lower, ymax = upper )) p + geom_hline(yintercept = 0, color = \u0026quot;gray80\u0026quot;) + geom_pointrange() + coord_flip() + labs(x = NULL, y = \u0026quot;Average Marginal Effect\u0026quot;) pv_cp \u0026lt;- cplot(out_bo, x = \u0026quot;sex\u0026quot;, draw = FALSE) ## xvals yvals upper lower ## 1 Male 0.5735849 0.6378653 0.5093045 ## 2 Female 0.6344507 0.6887845 0.5801169 p \u0026lt;- ggplot(data = pv_cp, aes( x = reorder(xvals, yvals), y = yvals, ymin = lower, ymax = upper )) p + geom_hline(yintercept = 0, color = \u0026quot;gray80\u0026quot;) + geom_pointrange() + coord_flip() + labs(x = NULL, y = \u0026quot;Conditional Effect\u0026quot;)  Plots for Surveys library(survey) ## Loading required package: grid ## Loading required package: Matrix ## ## Attaching package: \u0026#39;Matrix\u0026#39; ## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: ## ## expand, pack, unpack ## ## Attaching package: \u0026#39;survey\u0026#39; ## The following object is masked from \u0026#39;package:graphics\u0026#39;: ## ## dotchart library(srvyr) ## ## Attaching package: \u0026#39;srvyr\u0026#39; ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## filter options(survey.lonely.psu = \u0026quot;adjust\u0026quot;) options(na.action = \u0026quot;na.pass\u0026quot;) gss_wt \u0026lt;- subset(gss_lon, year \u0026gt; 1974) %\u0026gt;% mutate(stratvar = interaction(year, vstrat)) %\u0026gt;% as_survey_design( ids = vpsu, strata = stratvar, weights = wtssall, nest = TRUE ) out_grp \u0026lt;- gss_wt %\u0026gt;% filter(year %in% seq(1976, 2016, by = 4)) %\u0026gt;% group_by(year, race, degree) %\u0026gt;% summarize(prop = survey_mean(na.rm = TRUE)) # calculate properly weighted survey means ## Warning: Factor `degree` contains implicit NA, consider using ## `forcats::fct_explicit_na` out_mrg \u0026lt;- gss_wt %\u0026gt;% filter(year %in% seq(1976, 2016, by = 4)) %\u0026gt;% mutate(racedeg = interaction(race, degree)) %\u0026gt;% group_by(year, racedeg) %\u0026gt;% summarize(prop = survey_mean(na.rm = TRUE)) ## Warning: Factor `racedeg` contains implicit NA, consider using ## `forcats::fct_explicit_na` out_mrg \u0026lt;- gss_wt %\u0026gt;% filter(year %in% seq(1976, 2016, by = 4)) %\u0026gt;% mutate(racedeg = interaction(race, degree)) %\u0026gt;% group_by(year, racedeg) %\u0026gt;% summarize(prop = survey_mean(na.rm = TRUE)) %\u0026gt;% separate(racedeg, sep = \u0026quot;\\\\.\u0026quot;, into = c(\u0026quot;race\u0026quot;, \u0026quot;degree\u0026quot;))  ## Warning: Factor `racedeg` contains implicit NA, consider using ## `forcats::fct_explicit_na` p \u0026lt;- ggplot( data = subset(out_grp, race %nin% \u0026quot;Other\u0026quot;), mapping = aes( x = degree, y = prop, ymin = prop - 2 * prop_se, ymax = prop + 2 * prop_se, fill = race, color = race, group = race ) ) dodge \u0026lt;- position_dodge(width = 0.9) p + geom_col(position = dodge, alpha = 0.2) + geom_errorbar(position = dodge, width = 0.2) + scale_x_discrete(labels = scales::wrap_format(10)) + scale_y_continuous(labels = scales::percent) + scale_color_brewer(type = \u0026quot;qual\u0026quot;, palette = \u0026quot;Dark2\u0026quot;) + scale_fill_brewer(type = \u0026quot;qual\u0026quot;, palette = \u0026quot;Dark2\u0026quot;) + labs( title = \u0026quot;Educational Attainment by Race\u0026quot;, subtitle = \u0026quot;GSS 1976-2016\u0026quot;, fill = \u0026quot;Race\u0026quot;, color = \u0026quot;Race\u0026quot;, x = NULL, y = \u0026quot;Percent\u0026quot; ) + facet_wrap( ~ year, ncol = 2) + theme(legend.position = \u0026quot;top\u0026quot;) ## Warning: Removed 13 rows containing missing values (geom_col). ## Warning: Removed 13 rows containing missing values (geom_errorbar). p \u0026lt;- ggplot( data = subset(out_grp, race %nin% \u0026quot;Other\u0026quot;), mapping = aes( x = year, y = prop, ymin = prop - 2 * prop_se, ymax = prop + 2 * prop_se, fill = race, color = race, group = race ) ) p + geom_ribbon(alpha = 0.3, aes(color = NULL)) + #Use ribbon to show the error range geom_line() + #Use line to show a time trend facet_wrap( ~ degree, ncol = 1) + scale_y_continuous(labels = scales::percent) + scale_color_brewer(type = \u0026quot;qual\u0026quot;, palette = \u0026quot;Dark2\u0026quot;) + scale_fill_brewer(type = \u0026quot;qual\u0026quot;, palette = \u0026quot;Dark2\u0026quot;) + labs( title = \u0026quot;Educational Attainment by Race\u0026quot;, subtitle = \u0026quot;GSS 1976-2016\u0026quot;, fill = \u0026quot;Race\u0026quot;, color = \u0026quot;Race\u0026quot;, x = NULL, y = \u0026quot;Percent\u0026quot; ) + theme(legend.position = \u0026quot;top\u0026quot;) ## Warning: Removed 13 rows containing missing values (geom_path). Other useful packages: infer, ggally\n ","date":1569456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569486108,"objectID":"30974d6c28d1b290e7d11b670240fc6f","permalink":"/post/data-vis-chapter-6/","publishdate":"2019-09-26T00:00:00Z","relpermalink":"/post/data-vis-chapter-6/","section":"post","summary":"Show Several Fits at Once, with a Legend Model-based Graphics Tidy Model Objects with Broom get component-level statistics with tidy() Get observation-level statistics with augment()  Grouped Analysis Plots for Surveys   p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) p + geom_point(alpha = 0.1) + geom_smooth(color = \u0026quot;tomato\u0026quot;, fill = \u0026quot;tomato\u0026quot;, method = MASS::rlm) + #robust regression line geom_smooth(color = \u0026quot;steelblue\u0026quot;, fill = \u0026quot;steelblue\u0026quot;, method = \u0026quot;lm\u0026quot;) p + geom_point(alpha = 0.","tags":["socviz"],"title":"Data Vis Chapter 6","type":"post"},{"authors":[],"categories":["R","Books"],"content":"  Chapter 2 Chapter 3 Wrong way to set color Aesthetics Can Be Mapped per Geom Save plots  Chapter 4 Group data and the “Group” Aesthetic Facet to make small multiples Geoms can transform data Histgrams and Density Plots Avoid Transformations When Necessary    Chapter 2 geom_point\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point()  Chapter 3 geom_smooth\n## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point() + geom_smooth() ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; scale_x_log10\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point() + geom_smooth(method = \u0026quot;gam\u0026quot;) + scale_x_log10() scales::dollar\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point() + geom_smooth(method = \u0026quot;gam\u0026quot;) + scale_x_log10(labels = scales::dollar) Wrong way to set color p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = \u0026quot;purple\u0026quot;)) p + geom_point() + geom_smooth(method = \u0026quot;loess\u0026quot;) + scale_x_log10() The aes() function is for mappings only. Do not use it to change properties to a particular value. If we want to set a property, we do it in the geom_ we are using, and outside the mapping =aes(...)step.\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point(color = \u0026quot;purple\u0026quot;) + geom_smooth(method = \u0026quot;loess\u0026quot;) + scale_x_log10() The various geom_ functions can take many other arguments that will affect how the plot looks but do not involve mapping variables to aesthetic elements. “alpha” is an aesthetic property that points (and some other plot elements) have, and to which variables can be mapped. It controls how transparent the object will appear when drawn. It’s measured on a scale of zero to one.\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point(alpha = 0.3) + geom_smooth(color = \u0026quot;orange\u0026quot;, se = FALSE, size = 8, method = \u0026quot;lm\u0026quot;) + scale_x_log10() p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y=lifeExp)) p + geom_point(alpha = 0.3) + geom_smooth(method = \u0026quot;gam\u0026quot;) + scale_x_log10(labels = scales::dollar) + labs(x = \u0026quot;GDP Per Capita\u0026quot;, y = \u0026quot;Life Expectancy in Years\u0026quot;, title = \u0026quot;Economic Growth and Life Expectancy\u0026quot;, subtitle = \u0026quot;Data points are country-years\u0026quot;, caption = \u0026quot;Source: Gapminder.\u0026quot;) p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) p + geom_point() + geom_smooth(method = \u0026quot;loess\u0026quot;) + scale_x_log10() The color of the standard error ribbon is controlled by the fill aesthetic.\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, fill = continent)) p + geom_point() + geom_smooth(method = \u0026quot;loess\u0026quot;) + scale_x_log10()  Aesthetics Can Be Mapped per Geom p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point(mapping = aes(color = factor(year))) + geom_smooth(method = \u0026quot;loess\u0026quot;) + scale_x_log10() Order doesn’t matter!!! Besides scale_x_log10(), you can try scale_x_sqrt() and scale_x_reverse()\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = pop, y = lifeExp)) p + geom_smooth(method = \u0026quot;loess\u0026quot;) + geom_point(mapping = aes(color = continent)) + scale_x_reverse(labels = scales::number) p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point(mapping = aes(color = log(pop))) + scale_x_log10()  Save plots p_out \u0026lt;- p + geom_point() + geom_smooth(method = \u0026quot;loess\u0026quot;) + scale_x_log10() ggsave(\u0026quot;my_figure.pdf\u0026quot;, plot = p_out)   Chapter 4 Group data and the “Group” Aesthetic p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = year, y = gdpPercap)) p + geom_line() use the group aesthetic to tell ggplot explicitly about this country-level structure\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = year, y = gdpPercap)) p + geom_line(aes(group = country))  Facet to make small multiples use facet_wrap() to split our plot by continent.\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = year, y = gdpPercap)) p + geom_line(aes(group = country)) + facet_wrap(~continent) Add another enhancements\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = year, y = gdpPercap)) p + geom_line(color=\u0026quot;gray70\u0026quot;, aes(group = country)) + geom_smooth(size= 1.1, method = \u0026quot;loess\u0026quot;, se = FALSE) + scale_y_log10(labels=scales::dollar) + facet_wrap(~continent , ncol = 5) + labs(x = \u0026quot;Year\u0026quot;, y = \u0026quot;GDP per capita on Five Continents\u0026quot;) Use facet_grid\np \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = age, y = childs)) p + geom_point(alpha = 0.2) + geom_smooth() + facet_grid(sex ~ race) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; ## Warning: Removed 18 rows containing non-finite values (stat_smooth). ## Warning: Removed 18 rows containing missing values (geom_point). p \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = age, y = childs)) p + geom_point(alpha = 0.2) + geom_smooth() + facet_grid(sex ~ race + degree) ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39; ## Warning: Removed 18 rows containing non-finite values (stat_smooth). ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : span too small. fewer data values than degrees of freedom. ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : pseudoinverse used at 62.87 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : neighborhood radius 2.13 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : reciprocal condition number 0 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : There are other near singularities as well. 582.26 ## Warning in predLoess(object$y, object$x, newx = if ## (is.null(newdata)) object$x else if (is.data.frame(newdata)) ## as.matrix(model.frame(delete.response(terms(object)), : span too small. ## fewer data values than degrees of freedom. ## Warning in predLoess(object$y, object$x, newx = if ## (is.null(newdata)) object$x else if (is.data.frame(newdata)) ## as.matrix(model.frame(delete.response(terms(object)), : pseudoinverse used ## at 62.87 ## Warning in predLoess(object$y, object$x, newx = if ## (is.null(newdata)) object$x else if (is.data.frame(newdata)) ## as.matrix(model.frame(delete.response(terms(object)), : neighborhood radius ## 2.13 ## Warning in predLoess(object$y, object$x, newx = if ## (is.null(newdata)) object$x else if (is.data.frame(newdata)) ## as.matrix(model.frame(delete.response(terms(object)), : reciprocal ## condition number 0 ## Warning in predLoess(object$y, object$x, newx = if ## (is.null(newdata)) object$x else if (is.data.frame(newdata)) ## as.matrix(model.frame(delete.response(terms(object)), : There are other ## near singularities as well. 582.26 ## Warning: Removed 18 rows containing missing values (geom_point).  Geoms can transform data p \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = bigregion)) p + geom_bar() geom_bar called the default stat_ function associated with it,stat_count().\np \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = bigregion)) p + geom_bar(mapping = aes(y = ..prop..)) p \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = bigregion)) p + geom_bar(mapping = aes(y = ..prop.., group = 1)) table(gss_sm$religion) ## ## Protestant Catholic Jewish None Other ## 1371 649 51 619 159 p \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = religion, color = religion)) p + geom_bar() p \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = religion, fill = religion)) p + geom_bar() + guides(fill = FALSE) p + geom_bar() p \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = bigregion, fill = religion)) p + geom_bar() p \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = bigregion, fill = religion)) p + geom_bar(position = \u0026quot;fill\u0026quot;) if you want separate bars\np \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = bigregion, fill = religion)) p + geom_bar(position = \u0026quot;dodge\u0026quot;, mapping = aes(y = ..prop.., group = religion)) However, they don’t sum to one within each region. They sum to one across regions.\np \u0026lt;- ggplot(data = gss_sm, mapping = aes(x = religion)) p + geom_bar(position = \u0026quot;dodge\u0026quot;, mapping = aes(y = ..prop.., group = bigregion)) + facet_wrap(~bigregion, ncol=1)  Histgrams and Density Plots p \u0026lt;- ggplot(data = midwest, mapping = aes( x = area)) p + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. p \u0026lt;- ggplot(data = midwest, mapping = aes( x = area)) p + geom_histogram(bins = 10) oh_wi \u0026lt;- c(\u0026quot;OH\u0026quot;, \u0026quot;WI\u0026quot;) p \u0026lt;- ggplot(data = subset(midwest, subset = state %in% oh_wi), mapping = aes(x = percollege, fill = state)) p + geom_histogram(alpha = 0.4, bins = 20) p \u0026lt;- ggplot(data = midwest, mapping = aes( x = area)) p + geom_density() p \u0026lt;- ggplot(data = midwest, mapping = aes( x = area, fill = state, color = state)) p + geom_density(alpha = 0.3)  Avoid Transformations When Necessary p \u0026lt;- ggplot(data = titanic, mapping = aes(x = fate, y = percent, fill = sex)) p + geom_bar(position = \u0026quot;dodge\u0026quot;, stat = \u0026quot;identity\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) p \u0026lt;- ggplot(data = oecd_sum, mapping = aes(x = year, y = diff, fill = hi_lo)) p + geom_col() + guides(fill = FALSE) + labs(x = NULL, y = \u0026quot;Difference in Years\u0026quot;, title = \u0026quot;The US Life Expectancy Gap\u0026quot;, subtitle = \u0026quot;Difference between US and OECD average life expectancies, 1960-2015\u0026quot;, caption = \u0026quot;Data: OECD. After a chart by Christopher Ingraham, Washington Post, December 27th 2017.\u0026quot;) ## Warning: Removed 1 rows containing missing values (position_stack).   ","date":1569456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569427362,"objectID":"c74c6565458ff48f7ac5c534ac8cd2fa","permalink":"/post/test/","publishdate":"2019-09-26T00:00:00Z","relpermalink":"/post/test/","section":"post","summary":"Chapter 2 Chapter 3 Wrong way to set color Aesthetics Can Be Mapped per Geom Save plots  Chapter 4 Group data and the “Group” Aesthetic Facet to make small multiples Geoms can transform data Histgrams and Density Plots Avoid Transformations When Necessary    Chapter 2 geom_point\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point()  Chapter 3 geom_smooth","tags":["socviz"],"title":"Data Visualization Chapter 2-4","type":"post"},{"authors":null,"categories":["R","Books"],"content":"  Chapter 5 Use Pipes to Summerize Data  Continuous Variables by Group or Category Write and Draw in the Plot Area Scales, Guides, and Themes    Chapter 5 Use Pipes to Summerize Data rel_by_region \u0026lt;- gss_sm %\u0026gt;% group_by(bigregion, religion) %\u0026gt;% summarize(N = n()) %\u0026gt;% mutate(freq = N / sum(N), pct = round((freq*100), 0)) ## Warning: Factor `religion` contains implicit NA, consider using ## `forcats::fct_explicit_na` rel_by_region ## # A tibble: 24 x 5 ## # Groups: bigregion [4] ## bigregion religion N freq pct ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Northeast Protestant 158 0.324 32 ## 2 Northeast Catholic 162 0.332 33 ## 3 Northeast Jewish 27 0.0553 6 ## 4 Northeast None 112 0.230 23 ## 5 Northeast Other 28 0.0574 6 ## 6 Northeast \u0026lt;NA\u0026gt; 1 0.00205 0 ## 7 Midwest Protestant 325 0.468 47 ## 8 Midwest Catholic 172 0.247 25 ## 9 Midwest Jewish 3 0.00432 0 ## 10 Midwest None 157 0.226 23 ## # … with 14 more rows rel_by_region %\u0026gt;% group_by(bigregion) %\u0026gt;% summarize(total = sum(pct)) ## # A tibble: 4 x 2 ## bigregion total ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Northeast 100 ## 2 Midwest 101 ## 3 South 100 ## 4 West 101 p \u0026lt;- ggplot(rel_by_region, aes(x = bigregion, y = pct, fill = religion)) p + geom_col(position = \u0026quot;dodge2\u0026quot;) + labs(x = \u0026quot;Region\u0026quot;,y = \u0026quot;Percent\u0026quot;, fill = \u0026quot;Religion\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) Use coord_flip()\np \u0026lt;- ggplot(rel_by_region, aes(x = bigregion, y = pct, fill = religion)) p + geom_col(position = \u0026quot;dodge2\u0026quot;) + labs(x = \u0026quot;Region\u0026quot;,y = \u0026quot;Percent\u0026quot;, fill = \u0026quot;Religion\u0026quot;) + guides(fill = FALSE) + coord_flip() + facet_grid(~ bigregion) p \u0026lt;- ggplot(rel_by_region, aes(x = religion, y = pct, fill = religion)) p + geom_col(position = \u0026quot;dodge2\u0026quot;) + labs(x = NULL,y = \u0026quot;Percent\u0026quot;, fill = \u0026quot;Religion\u0026quot;) + guides(fill = FALSE) + coord_flip() + facet_grid(~ bigregion)   Continuous Variables by Group or Category p \u0026lt;- ggplot(data = organdata, mapping = aes(x = year, y = donors)) p + geom_line(aes(group = country)) + facet_wrap(~country) ## Warning: Removed 34 rows containing missing values (geom_path). p \u0026lt;- ggplot(data = organdata, mapping = aes(x = country, y = donors)) p + geom_boxplot() + coord_flip() ## Warning: Removed 34 rows containing non-finite values (stat_boxplot). p \u0026lt;- ggplot(data = organdata, mapping = aes(x = reorder(country, donors, na.rm = TRUE), y = donors)) p + geom_boxplot() + labs(x = NULL) + coord_flip() ## Warning: Removed 34 rows containing non-finite values (stat_boxplot). p \u0026lt;- ggplot(data = organdata, mapping = aes(x = reorder(country, donors, na.rm = TRUE), y = donors, fill = world)) p + geom_boxplot() + labs(x = NULL) + coord_flip() + theme(legend.position = \u0026quot;top\u0026quot;) ## Warning: Removed 34 rows containing non-finite values (stat_boxplot). p \u0026lt;- ggplot(data = organdata, mapping = aes(x = reorder(country, donors, na.rm = TRUE), y = donors, color = world)) p + geom_point() + labs(x = NULL) + coord_flip() + theme(legend.position = \u0026quot;top\u0026quot;) ## Warning: Removed 34 rows containing missing values (geom_point). p \u0026lt;- ggplot(data = organdata, mapping = aes( x = reorder(country, donors, na.rm = TRUE), y = donors, color = world )) p + geom_jitter() + labs(x = NULL) + coord_flip() + theme(legend.position = \u0026quot;top\u0026quot;) ## Warning: Removed 34 rows containing missing values (geom_point). p \u0026lt;- ggplot(data = organdata, mapping = aes( x = reorder(country, donors, na.rm = TRUE), y = donors, color = world )) p + geom_jitter(position = position_jitter(width = 0.15)) + labs(x = NULL) + coord_flip() + theme(legend.position = \u0026quot;top\u0026quot;) ## Warning: Removed 34 rows containing missing values (geom_point). by_country \u0026lt;- organdata %\u0026gt;% group_by(consent_law, country) %\u0026gt;% summarize( donors_mean = mean(donors, na.rm = TRUE), donors_sd = sd(donors, na.rm = TRUE), gdp_mean = mean(gdp, na.rm = TRUE), health_mean = mean(health, na.rm = TRUE), roads_mean = mean(roads, na.rm = TRUE), cerebvas_mean = mean(cerebvas, na.rm = TRUE) ) by_country ## # A tibble: 17 x 8 ## # Groups: consent_law [2] ## consent_law country donors_mean donors_sd gdp_mean health_mean ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Informed Austra… 10.6 1.14 22179. 1958. ## 2 Informed Canada 14.0 0.751 23711. 2272. ## 3 Informed Denmark 13.1 1.47 23722. 2054. ## 4 Informed Germany 13.0 0.611 22163. 2349. ## 5 Informed Ireland 19.8 2.48 20824. 1480. ## 6 Informed Nether… 13.7 1.55 23013. 1993. ## 7 Informed United… 13.5 0.775 21359. 1561. ## 8 Informed United… 20.0 1.33 29212. 3988. ## 9 Presumed Austria 23.5 2.42 23876. 1875. ## 10 Presumed Belgium 21.9 1.94 22500. 1958. ## 11 Presumed Finland 18.4 1.53 21019. 1615. ## 12 Presumed France 16.8 1.60 22603. 2160. ## 13 Presumed Italy 11.1 4.28 21554. 1757 ## 14 Presumed Norway 15.4 1.11 26448. 2217. ## 15 Presumed Spain 28.1 4.96 16933 1289. ## 16 Presumed Sweden 13.1 1.75 22415. 1951. ## 17 Presumed Switze… 14.2 1.71 27233 2776. ## # … with 2 more variables: roads_mean \u0026lt;dbl\u0026gt;, cerebvas_mean \u0026lt;dbl\u0026gt; by_country \u0026lt;- organdata %\u0026gt;% group_by(consent_law, country) %\u0026gt;% summarize_if(is.numeric, lst(mean, sd), na.rm = TRUE) %\u0026gt;% ungroup() by_country ## # A tibble: 17 x 28 ## consent_law country donors_mean pop_mean pop_dens_mean gdp_mean ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Informed Austra… 10.6 18318. 0.237 22179. ## 2 Informed Canada 14.0 29608. 0.297 23711. ## 3 Informed Denmark 13.1 5257. 12.2 23722. ## 4 Informed Germany 13.0 80255. 22.5 22163. ## 5 Informed Ireland 19.8 3674. 5.23 20824. ## 6 Informed Nether… 13.7 15548. 37.4 23013. ## 7 Informed United… 13.5 58187. 24.0 21359. ## 8 Informed United… 20.0 269330. 2.80 29212. ## 9 Presumed Austria 23.5 7927. 9.45 23876. ## 10 Presumed Belgium 21.9 10153. 30.7 22500. ## 11 Presumed Finland 18.4 5112. 1.51 21019. ## 12 Presumed France 16.8 58056. 10.5 22603. ## 13 Presumed Italy 11.1 57360. 19.0 21554. ## 14 Presumed Norway 15.4 4386. 1.35 26448. ## 15 Presumed Spain 28.1 39666. 7.84 16933 ## 16 Presumed Sweden 13.1 8789. 1.95 22415. ## 17 Presumed Switze… 14.2 7037. 17.0 27233 ## # … with 22 more variables: gdp_lag_mean \u0026lt;dbl\u0026gt;, health_mean \u0026lt;dbl\u0026gt;, ## # health_lag_mean \u0026lt;dbl\u0026gt;, pubhealth_mean \u0026lt;dbl\u0026gt;, roads_mean \u0026lt;dbl\u0026gt;, ## # cerebvas_mean \u0026lt;dbl\u0026gt;, assault_mean \u0026lt;dbl\u0026gt;, external_mean \u0026lt;dbl\u0026gt;, ## # txp_pop_mean \u0026lt;dbl\u0026gt;, donors_sd \u0026lt;dbl\u0026gt;, pop_sd \u0026lt;dbl\u0026gt;, pop_dens_sd \u0026lt;dbl\u0026gt;, ## # gdp_sd \u0026lt;dbl\u0026gt;, gdp_lag_sd \u0026lt;dbl\u0026gt;, health_sd \u0026lt;dbl\u0026gt;, health_lag_sd \u0026lt;dbl\u0026gt;, ## # pubhealth_sd \u0026lt;dbl\u0026gt;, roads_sd \u0026lt;dbl\u0026gt;, cerebvas_sd \u0026lt;dbl\u0026gt;, ## # assault_sd \u0026lt;dbl\u0026gt;, external_sd \u0026lt;dbl\u0026gt;, txp_pop_sd \u0026lt;dbl\u0026gt; p \u0026lt;- ggplot(data = by_country, mapping = aes( x = donors_mean, y = reorder(country, donors_mean), color = consent_law )) p + geom_point(size = 3) + labs(x = \u0026quot;Donor Procurement Rate\u0026quot;, y = \u0026quot;\u0026quot;, color = \u0026quot;Consent Law\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) p \u0026lt;- ggplot(data = by_country, mapping = aes(x = donors_mean, y = reorder(country, donors_mean))) p + geom_point(size = 3) + facet_wrap( ~ consent_law, scales = \u0026quot;free_y\u0026quot;, ncol = 1) + labs(x = \u0026quot;Donor Procurement Rate\u0026quot;, y = \u0026quot;\u0026quot;) p \u0026lt;- ggplot(data = by_country, mapping = aes(x = reorder(country, donors_mean), y = donors_mean)) p + geom_pointrange(mapping = aes(ymin = donors_mean - donors_sd, ymax = donors_mean + donors_sd)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Donor Procurement Rate\u0026quot;) + coord_flip() ### Plot Text Directly\np \u0026lt;- ggplot(data = by_country, mapping = aes(x = roads_mean, y = donors_mean)) p + geom_point() + geom_text(mapping = aes(label = country)) p \u0026lt;- ggplot(data = by_country, mapping = aes(x = roads_mean, y = donors_mean)) p + geom_point() + geom_text(mapping = aes(label = country), hjust = 0) ggrepel is better than geom_text()\nlibrary(ggrepel) p_title \u0026lt;- \u0026quot;Presidential Elections: Popular \u0026amp; Electoral College Margins\u0026quot; p_subtitle \u0026lt;- \u0026quot;1824-2016\u0026quot; p_caption \u0026lt;- \u0026quot;Data for 2016 are provisional.\u0026quot; x_label \u0026lt;- \u0026quot;Winner\u0026#39;s share of Popular Vote\u0026quot; y_label \u0026lt;- \u0026quot;Winner\u0026#39;s share of Electoral College Votes\u0026quot; p \u0026lt;- ggplot(elections_historic, aes(x = popular_pct, y = ec_pct, label = winner_label)) p + geom_hline(yintercept = 0.5, size = 1.4, color = \u0026quot;gray80\u0026quot;) + geom_vline(xintercept = 0.5, size = 1.4, color = \u0026quot;gray80\u0026quot;) + geom_point() + geom_text_repel() + scale_x_continuous(labels = scales::percent) + scale_y_continuous(labels = scales::percent) + labs( x = x_label, y = y_label, title = p_title, subtitle = p_subtitle, caption = p_caption ) ### Label Outliers\np \u0026lt;- ggplot(data = by_country, mapping = aes(x = gdp_mean, y = health_mean)) p + geom_point() + geom_text_repel(data = subset(by_country, gdp_mean \u0026gt; 25000), mapping = aes(label = country)) p \u0026lt;- ggplot(data = by_country, mapping = aes(x = gdp_mean, y = health_mean)) p + geom_point() + geom_text_repel( data = subset( by_country, gdp_mean \u0026gt; 25000 | health_mean \u0026lt; 1500 | country %in% \u0026quot;Belgium\u0026quot; ), mapping = aes(label = country) ) organdata$ind \u0026lt;- organdata$ccode %in% c(\u0026quot;Ita\u0026quot;, \u0026quot;Spa\u0026quot;) \u0026amp; organdata$year \u0026gt; 1998 p \u0026lt;- ggplot(data = organdata, mapping = aes(x = roads, y = donors, color = ind)) p + geom_point() + geom_text_repel(data = subset(organdata, ind), mapping = aes(label = ccode)) + guides(label = FALSE, color = FALSE) ## Warning: Removed 34 rows containing missing values (geom_point). Write and Draw in the Plot Area p \u0026lt;- ggplot(data = organdata, mapping = aes(x = roads, y = donors)) p + geom_point() + annotate( geom = \u0026quot;text\u0026quot;, x = 91, y = 33, label = \u0026quot;A surprisingly high \\n recovery rate.\u0026quot;, hjust = 0 ) ## Warning: Removed 34 rows containing missing values (geom_point). p \u0026lt;- ggplot(data = organdata, mapping = aes(x = roads, y = donors)) p + geom_point() + annotate( geom = \u0026quot;rect\u0026quot;, xmin = 125, xmax = 155, ymin = 30, ymax = 35, fill = \u0026quot;red\u0026quot;, alpha = 0.2 ) + annotate( geom = \u0026quot;text\u0026quot;, x = 157, y = 33, label = \u0026quot;A surprisingly high \\n recovery rate.\u0026quot;, hjust = 0 ) ## Warning: Removed 34 rows containing missing values (geom_point).  Scales, Guides, and Themes p \u0026lt;- ggplot(data = organdata, mapping = aes(x = roads, y = donors, color = world)) p + geom_point() ## Warning: Removed 34 rows containing missing values (geom_point). p \u0026lt;- ggplot(data = organdata, mapping = aes(x = roads, y = donors, color = world)) p + geom_point() + scale_x_log10() + scale_y_continuous(breaks = c(5, 15, 25), labels = c(\u0026quot;Five\u0026quot;, \u0026quot;Fifteen\u0026quot;, \u0026quot;Twenty Five\u0026quot;)) ## Warning: Removed 34 rows containing missing values (geom_point). p \u0026lt;- ggplot(data = organdata, mapping = aes(x = roads, y = donors, color = world)) p + geom_point() + scale_color_discrete(labels = c(\u0026quot;Corporatist\u0026quot;, \u0026quot;Liberal\u0026quot;, \u0026quot;Social Democratic\u0026quot;, \u0026quot;Unclassified\u0026quot;)) + labs(x = \u0026quot;Road Deaths\u0026quot;, y = \u0026quot;Donor Procurement\u0026quot;, color = \u0026quot;Welfare State\u0026quot;) ## Warning: Removed 34 rows containing missing values (geom_point). p \u0026lt;- ggplot(data = organdata, mapping = aes(x = roads, y = donors, color = world)) p + geom_point() + labs(x = \u0026quot;Road Deaths\u0026quot;, y = \u0026quot;Donor Procurement\u0026quot;) + guides(color = FALSE) ## Warning: Removed 34 rows containing missing values (geom_point).   ","date":1569456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569428735,"objectID":"7f95c8023fb78969915abcbd29b7910d","permalink":"/post/data-visualization-chapter-5/","publishdate":"2019-09-26T00:00:00Z","relpermalink":"/post/data-visualization-chapter-5/","section":"post","summary":"Chapter 5 Use Pipes to Summerize Data  Continuous Variables by Group or Category Write and Draw in the Plot Area Scales, Guides, and Themes    Chapter 5 Use Pipes to Summerize Data rel_by_region \u0026lt;- gss_sm %\u0026gt;% group_by(bigregion, religion) %\u0026gt;% summarize(N = n()) %\u0026gt;% mutate(freq = N / sum(N), pct = round((freq*100), 0)) ## Warning: Factor `religion` contains implicit NA, consider using ## `forcats::fct_explicit_na` rel_by_region ## # A tibble: 24 x 5 ## # Groups: bigregion [4] ## bigregion religion N freq pct ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Northeast Protestant 158 0.","tags":["socviz"],"title":"Data Visualization Chapter 5","type":"post"},{"authors":[],"categories":["Books"],"content":" 本书第一章主要对一些术语进行了界定，把元分析同其它种文献综述的方式进行了区分。元分析同其它定性的总结以及定量的（Informal vote counting-一般采用多数原则来总结结论与formal vote counting-在前者基础之上采用了一些统计分析以期得到统计上显著的结论）一些分析的不同之处在于：元分析的关注点除了关注效果是否存在之外，主要关注效果的大小(effect size)。\n元分析的工作步骤分为五个阶段：\n确定问题(formulate a problem) 在确定问题开始综述工作时，要把关注的重点厘清。比如希望是一个更概括的结论或样本，还是一个适用范围有限定的结论和样本，这将决定第二三阶段对文献的取舍。\n取得相关文献 要从尽可能多的样本里采样，确保文献是有代表性的(representive)和无偏的(unbiased)。后者因为学术刊物对于结果显著的论文的发表偏好而很难实现，因此别忘了未发表的工作论文或毕业论文等。\n对文献进行评估精选 本阶段对上一阶段取得的文献进行相关性评估。在此阶段会对第一阶段确定下来的研究问题进行进一步的提炼。\n对文献进行分析和解释 最花时间和最难的阶段，在此阶段中需要将文献的数据整理和输入。\n文献综述的写作 有几点需要注意的：\n 对于整个文献综述的工作过程要完整描述，记录在此过程中所做的各项取舍 关键在与要回答感兴趣的问题，假如不能回答，也要解释为什么以及未来需要做什么来回答这个问题 要避免文献列表的堆砌  作者的几点建议 因为文献的收集、评估、整理直到分析很花时间和精力，所以一定要做到有规划。一些具体的建议如下：\n 在文献收集过程中做记录 系统的存放文献，假如同别人合作，确保你们用同一套系统来存放，处理文献  我对于工具的建议 文献收集可以用Mendeley的private group功能，这样加入同一组的成员可以直接在客户端上打开PDF和加标注。虽然mendeley对免费用户private group数有限制，但可以通过在group底下再加子目录的方式来绕过限制。\n最新的建议 转向Zotero\n","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569402478,"objectID":"e955fdc8baf2a3e3ad84d5ce735d14d5","permalink":"/post/meta-analysis-note-1/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/post/meta-analysis-note-1/","section":"post","summary":"本书第一章主要对一些术语进行了界定，把元分析同其它种文献综述的方式进行了区分。元分析同其它定性的总结以及定量的（Informal vote count","tags":["Meta"],"title":"Meta-Analysis Note 1","type":"post"},{"authors":[],"categories":["Stata"],"content":" SEM sem bmi \u0026lt;- age children incomeln educ quickfood  This would give us the unstandardized solution. This command uses maximum likelihood estimation ather than the ordinary least-squares (OLS) estimation used by the regress command. Add ,standardized just like add ,beta to regress\noption method(mlmv) (maximum likelihood with missing values): Estimation is less robust to the assumption of multivariate normality when using the method(mlmv) option than when using maximum likelihood estimation with listwise deletion of observations with missing values. Because some of the five variables in our model are not normally distributed, the method(mlmv) option needs to be used with caution. The estimation performed when we use the method(mlmv) option also assumes that the missing values are MAR1 . By contrast, when listwise deletion is used we are assuming that missing values are MCAR1, and this is a much more restrictive assumption.\nsem bmi \u0026lt;- age children incomeln educ quickfood, method(mlmv) standardized estat eqgof  The OLS regression solution and the SEM solution without MLMV, which uses listwise deletion, are producing the same standardized parameter estimates and $R^2$s. As noted, the z values are slightly larger than the t-values, and the p-values are slightly smaller. The z tests for the SEM solution are directly testing the standardized solution. The regress solution’s t tests are testing the significance of the unstandardized B coefficients and do not directly test the significance of the Betas. The regress command does not provide such a direct test for the significance of Betas.\nNotice that the $R^2$ using sem with method(mlmv) is actually slightly smaller. Using all the available information in the SEM solution with MLMV is not cheating if the assumptions are met. The MAR assumption for the SEM solution is more realistic than the MCAR assumption required for listwise deletion to be unbiased.\nThere are three rules to follow when using the maximum likelihood with missing values estimation.\n Generate an indicator variable for each variable in your model to reflect whether an observation has a missing value. Correlate potential auxiliary variables to see whether they predict missing value indicator variables. Include additional auxiliary variables that are substantially correlated with a person’s score on a variable that has missing values.  Getting auxiliary variables into your SEM command？？？没懂\nGSEM logit obese age children incomeln educ quickfood listcoef glm obese age children incomeln educ quickfood, family(binomial) link(logit) glm, eform  The logit command is a special application of the generalized linear model. We can obtain the same results by using the glm command. The glm command requires us to specify the family of our model, family(binomial), and the link function, link(logit). To obtain the odds ratio, we can replay these results by using glm, eform.\n后面没看懂，以后再说吧。\n Missing Completely at Random is pretty straightforward. What it means is what is says: the propensity for a data point to be missing is completely random. ^   ","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569402124,"objectID":"3bfa4b9feed160266ff86cde330da45d","permalink":"/post/sem-and-gsem/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/post/sem-and-gsem/","section":"post","summary":"SEM sem bmi \u0026lt;- age children incomeln educ quickfood This would give us the unstandardized solution. This command uses maximum likelihood estimation ather than the ordinary least-squares (OLS) estimation used by the regress command. Add ,standardized just like add ,beta to regress option method(mlmv) (maximum likelihood with missing values): Estimation is less robust to the assumption of multivariate normality when using the method(mlmv) option than when using maximum likelihood estimation","tags":["Stata"],"title":"SEM and GSEM","type":"post"},{"authors":[],"categories":["R","Stata"],"content":" Panel data with one way fixed effect mm1 \u0026lt;- invforward ~ TOBINQ + inv + top3 + size + lev + cash + loss + lnage + cfo + sd + ic + factor(year) zzz \u0026lt;- plm(mm1,data=sample,model=\u0026quot;within\u0026quot;,index=c(\u0026quot;stkcd\u0026quot;))  same as xtreg i.year fe , without robust vcetype 用这种方法算出来$R^2$和Stata报告$R^2$ within的一致\nm1 \u0026lt;- invforward ~ TOBINQ + inv + top3 + size + lev + cash + loss + lnage + cfo + sd + ic zz \u0026lt;- plm(m1,data=sample,model=\u0026quot;within\u0026quot;,index=c(\u0026quot;stkcd\u0026quot;, \u0026quot;year\u0026quot;),effect = \u0026quot;twoways\u0026quot;) summary(zz)  same sa xtreg i.year, fe , without robust vcetype，但$R^2$较Stata报告$R^2$ within小\nvcetype robust zz_r \u0026lt;- coeftest(zz, vcov.=function(x) vcovHC(x, type=\u0026quot;sss\u0026quot;)) # same as stata xtreg i.year, fe r # OR zzz_r \u0026lt;- coeftest(zzz, vcov.=function(x) vcovHC(x, type=\u0026quot;sss\u0026quot;))  组间系数比较 OLS可用\nsur_diff \u0026lt;- MVBV ~ (Dm + Dh + EBV + DmEBV +DhEBV)*g_layer h2t \u0026lt;- h2 %\u0026gt;% filter(g_layer != 2)%\u0026gt;% mutate(g_layer = ifelse(g_layer == 1, 0, 1)) mm \u0026lt;- lm(sur_diff,data=h2t) ttt \u0026lt;- coeftest(mm, vcov.=function(x) vcovHC(x, cluster=\u0026quot;group\u0026quot;, type=\u0026quot;HC1\u0026quot;)) stargazer(fpm,models_growth_layer,type = \u0026quot;text\u0026quot;, column.labels = table4_label) stargazer(fpm_r,robusts_growth_layer,type = \u0026quot;text\u0026quot;, column.labels = table4_label, add.lines=c(\u0026quot;DhEBV(4)-(2)\u0026quot;, str_c(round(ttt[12,1],3),\u0026quot;**(p=\u0026quot;,round(ttt[12,4],3),\u0026quot;)\u0026quot;)))  Panel Data不行！One way, two way fixed effect都不行！ 建议直接加interaction\n","date":1566864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569396028,"objectID":"bada11b7c33d5bda7e4252844f18fd1e","permalink":"/post/panel-data-in-r-vs-in-stata/","publishdate":"2019-08-27T00:00:00Z","relpermalink":"/post/panel-data-in-r-vs-in-stata/","section":"post","summary":"Panel data with one way fixed effect mm1 \u0026lt;- invforward ~ TOBINQ + inv + top3 + size + lev + cash + loss + lnage + cfo + sd + ic + factor(year) zzz \u0026lt;- plm(mm1,data=sample,model=\u0026quot;within\u0026quot;,index=c(\u0026quot;stkcd\u0026quot;)) same as xtreg i.year fe , without robust vcetype 用这种方法算出来$R^2$和Stata报告$R^","tags":["R","Stata"],"title":"Panel data in R vs in Stata","type":"post"},{"authors":[],"categories":["R"],"content":"效應評估模型 “提高最低工資是否會減少就業？”\n“最低工資提高是否餐廳的全職員工數會減少？”\n假設 $MinWage$為「最低工資有提高」的虛擬變數， $FEmp$為餐廳全職員工數。\n\\[ FEmp_i=FEmp_{0,i}+\\beta^*MinWage_i \\]\n\\[ FEmp_i=\\beta_0+\\beta_1 MinWage_i+\\epsilon_i \\]\n「沒有受到最低工資提高影響下的員工數」$FEmp_{0,i}$與「有無受到最低工資提高影響」无關时OLS是一致估计。\n令 $s$表示餐廳所屬的州，則原本的效應模型可以寫成： \\( \\begin{eqnarray} FEmp_{is}=FEmp_{0,is}+\\beta^*MinWage_{s} \\tag{7.1} \\end{eqnarray} \\)\n    Pre Post     Control  $MinWage=1$:PA   Treatment  $MinWage=1$:NJ    複迴歸模型 餐廳的型態（大型連鎖、咖啡店、小吃店等等）會影響員工僱用量。 \\( \\begin{eqnarray} FEmp_{is} =FEmp_{0,-type,is}+\\beta^*MinWage_s+\\gamma'type_{is} \\tag{7.2} \\end{eqnarray} \\) 其中 \\( FEmp_{0,-type,is}=FEmp_{0,is}-\\mathbb{E}(FEmp_{0,is}|type_{is}) \\)\n 在思考怱略變數偏誤(omitted variable bias)時，可能的confounder都必需放在（依實驗組/控制組分的）加總層級來思考。\n 固定效果 組固定效果 \\[ FEmp_{is}=FEmp_{0,is}+\\beta^*MinWage_{s} \\]\n多數時候實驗組/控制組在政策還沒施行前，他們就存在組間的特質差異，也就是 \\( FEmp_{0,is}=FEmp_{0,-\\alpha_s,is}+\\alpha_s \\) 其中$\\alpha_s$ 代表因組而異的confounder效果。\n若沒有其他confounder，我們可以估計以下迴歸模型： \\( FEmp_{ist}=\\alpha_s+\\beta^* MinWage_{st}+\\epsilon_{ist} \\)\n時間固定效果 \\[ FEmp_{ist}=FEmp_{0,-(\\alpha_s,\\delta_t),ist}+\\alpha_s+\\delta_t+\\beta^*MinWage_{st} \\]\n所對應的迴歸模型為： \\( FEmp_{ist}=\\alpha_s+\\delta_t+\\beta^* MinWage_{st}+\\epsilon_{ist} \\)\n資料追踪/不追踪 雖然$FEmp_{ist}$ 有到個別餐廳（即有下標 $i$），然而固定效果只到組層級（即下標 $s$)，因此在估計上我們並不需要追踪同一家餐廳——各期抽樣的餐廳可以不同。\nDiD 估计法 \\[ \\begin{eqnarray} FEmp_{ist}=\\alpha_s+\\delta_t+\\beta^*MinWage_{st}+\\epsilon_{ist} \\tag{7.3} \\end{eqnarray} \\]\n\\[ FEmp_{ist}=\\beta_0+\\alpha_1D1_s+\\delta_1B1_t+\\beta_1MinWage_{st}+\\epsilon_{ist} \\]\n 令$D1=1$代表來自第1個州（NJ）的虛擬變數。 令$B1 = 1$代表政策施行「後」的虛擬變數。 $MinWage_{st}=D1_s\\times B1_t$     State t=0 T=1     NJ D1=1,B1=0 D1=1,B1=1   PA D1=0,B1=0 D1=0,B1=1    cluster standard error 我們有G1-G4共四群誤差項的變異數及跨群間的共變異數需要去留意，當誤差項有聚類（clustering）可能時，必需要適當的調整估計式標準誤。\n","date":1562716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569398122,"objectID":"605816064d436cbe1bbd1b41dacca689","permalink":"/post/difference-in-difference/","publishdate":"2019-07-10T00:00:00Z","relpermalink":"/post/difference-in-difference/","section":"post","summary":"效應評估模型 “提高最低工資是否會減少就業？” “最低工資提高是否餐廳的全職員工數會減少？” 假設 $MinWage$為「最低工資有提高」的虛擬變數","tags":["econometrics-with-r"],"title":"Difference in Difference","type":"post"},{"authors":[],"categories":["R"],"content":"效應評估模型 \\[ mrall=mrall_{-BeerTax}+\\beta^*BeerTax \\]\n提高啤酒稅（BeerTax）是否有助減低車禍死亡率（mrall）？\n固定效應模型 令 $W$代表「州愛喝酒程度」。\n $W$與 $mrall_{-BeerTax}+$有關 $W$與 $BeerTax$有關  \\[ mrall=(mrall_{-BT}-\\mathbb{E}(mrall_{-BT}|W))+\\mathbb{E}(mrall_{-BT}|W) + \\beta^*BeerTax \\]\n\\[ mrall_{-BT,-W}\\equiv mrall_{-BT}-\\mathbb{E}(mrall_{-BT}|W) \\]\n\\[ mrall=mrall_{-BT,-W}+\\mathbb{E}(mrall_{-BT}|W)+\\beta^*BeerTax \\]\n$mrall_{-BT,-W}$為「去除」 $W$影響的「非啤酒稅造成的車禍死亡因素」：\n 它與 $W$無關。 若兩筆obs有相同飲酒文化，即$W$相同，他們的 $\\mathbb{E}(mrall_{-BT}|W)$ 會相同。   「假設」一個地方的飲酒文化「不隨時間改變」，即同一州在不同時點的$W$相同。\n 令\\(\\mathbb{E}(mrall_{-BT,it}|W_i)=\\alpha_i\\)， 故我們的效應模型可以寫成： \\( mrall_{it}=mrall_{-BT,-W,it}+\\alpha_i+\\beta^*BeerTax_{it} \\) 其中$\\alpha_i$為第 $i$ 個州的固定效果：\n $BearTax$與$mrall_{-BT,-W}$無關 $BearTax$與$\\alpha$有關  組內差異最小平方法 差分OLS解决$\\alpha_i$不可得的阻碍\n\\[ mrall_{i1}-mrall_{i0}=\\beta^* (BeerTax_{i1}-BearTax_{i0})+(mrall_{-BT,-W,i1}-mrall_{-BT,-W,i0}) \\]\n如果$t$超過兩期，考慮用組內平均為差分比較的點。\n即\\(x_1-\\bar{x},x_2-\\bar{x},...,x_n-\\bar{x}, \\bar{x}=\\sum_{i=1}^n x_i/n\\) \\( \\bar{mrall}_i=\\sum_{t=1}^T mrall_{it}/T \\\\ \\bar{BeerTax}_i=\\sum_{t=1}^T BeerTax_{it}/T\\\\ \\bar{mrall}_{-BT,-W,i}=\\sum_{t=1}^T mrall_{-BT,-W,it}/T \\)\n\\[ mrall_{it}-\\bar{mrall}_i=\\beta^*\\left( BeerTax_{it}-\\bar{BeerTax}_i\\right)+(mrall_{-BT,-W,it}-\\bar{mrall}_{-BT,-W,i}) \\]\n固定效果模型下，我們可以以最小平方法估計下面的迴歸式： \\( mrall_{it}-\\bar{mrall}_i=\\beta_0+\\beta_1\\left( BeerTax_{it}-\\bar{BeerTax}_i\\right)+\\epsilon_{it} \\) 其中$\\hat{\\beta}_1$即為$\\beta^*$的一致性估計\n常見的固定效果模型  Identity fixed effect:$\\alpha_i$ Time fixed effect: $\\delta_i$  \\[ mrall_{-BT,it}=mrall_{-BT,-W_i,-Z_t}+\\alpha_i+\\delta_t \\]\n $W_i$為造成效應係數估計偏誤的變數，它在$i$面向固定不變。 $Z_t$為造成效應係數估計偏誤的變數，它在$t$面向固定不變。   如$Z_t$為全美國的景氣狀況。\n 對應的迴歸模型： \\( mrall_{it}=\\alpha_i+\\delta_t+\\beta_1 BeerTax_{it}+\\epsilon_{it} \\)\n廣義的固定效果模型 \\[ mrall=mrall_{-BeerTax}+\\beta^*BeerTax \\]\n但 \\( \\begin{equation} mrall_{-BT,it}\\not\\perp BeerTax_{it} \\tag{5.1} \\end{equation} \\)\n複迴歸控制 先思考造成(5.1)的變數有哪些——統計上稱這些變數為混淆變數(confounder)。Confounder中有資料的（令為$Z$）可進一步用來擴充模型成為： \\( mrall_{it}=mrall_{-BT,-Z,it}+\\beta^*BeerTax_{it}+\\gamma'Z_{it} \\) 其中： \\( mrall_{-BT,-Z}=mrall_{-BT}-\\mathbb{E}(mrall_{-BT}|Z) \\)\n固定效果模型 Confounder中沒有資料但在某些面向固定的，假設分成以下兩類：\n $W_i$：在同個identity下固定。 $V_t$：在同個time下固定。  \\[ \\begin{eqnarray} mrall_{it}=mrall_{-BT,-(Z,W,V),it}+\\beta^*BeerTax_{it}+\\\\ \\alpha_i+\\delta_t+\\gamma'Z_{it} \\tag{5.2} \\end{eqnarray} \\]\n(5.2)是相當廣義的固定效果效應模型——有兩個面向的固定效果及控制變數。\n隨機效果模型 \\[ mrall_{it}=mrall_{-BT,-Z,it}+\\beta^*BeerTax_{it}+\\gamma'Z_{it} \\]\n隨機效果模型(Random Effect model)的設定：\n 使用迴歸模型：  \\[ \\begin{eqnarray} mrall_{it}=\\beta_0+\\beta_{1}BeerTax_{it}+\\gamma'Z_{it}+\\nu_{it} \\tag{5.3} \\end{eqnarray} \\]\n 假設$\\nu_{it}$ 具有某種結構。  其中假设：\n $\\nu_{it}\\perp BeerTax_{it}$ \\(var(\\alpha_i|X)=\\sigma_{\\alpha}^2\\) $var(\\epsilon_{it}|X)=\\sigma^2$ $cov(\\epsilon_{it},\\epsilon_{is}|X)=0$   隨機效果模型帶有高度誤差項假設，故不建議使用。\n Hausman檢定 固定效果模型(FE) 表示使用組內差異最小平法方去估算以下迴歸模型中的\\(\\beta_1\\): \\( mrall_{it}=\\beta_0+\\beta_{1}BeerTax_{it}+\\gamma'Z_{it}+\\alpha_i+\\epsilon_{it} \\)\n隨機效果模型(RE) 表示使用GLS去估算以下迴歸模型中的\\(\\beta_1\\): \\( mrall_{it}=\\beta_0+\\beta_{1}BeerTax_{it}+\\gamma'Z_{it}+\\nu_{it} \\)\n \\(\\nu_{it}=\\alpha_i+\\epsilon_{it}\\)  假設\n RE下「關於variance、covariance的假設」都成立。 \\(\\epsilon_{it} \\perp BeerTax_{it} | \\alpha_i,Z_{it}\\)  H0: \\(\\alpha_i \\perp BeerTax_{it} |Z_{it}\\)\nH0为RE，拒绝则为FE\n","date":1562716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569398496,"objectID":"416a740a41f6f1e85a0b27c05996b70f","permalink":"/post/panel-data/","publishdate":"2019-07-10T00:00:00Z","relpermalink":"/post/panel-data/","section":"post","summary":"效應評估模型 \\[ mrall=mrall_{-BeerTax}+\\beta^*BeerTax \\] 提高啤酒稅（BeerTax）是否有助減低車禍死亡率（mrall）？ 固定效應模型 令 $W$代表「州愛喝酒程度」。 $W$與 $mr","tags":["econometrics-with-r"],"title":"Panel Data","type":"post"},{"authors":[],"categories":["R"],"content":"OLS estimator The method to compute (or estimate) $b_0$ and $b_1$ we illustrated above is called Ordinary Least Squares, or OLS. $b_0$ and $b_1$ are therefore also often called the OLS coefficients. By solving problem\n\\[ \\begin{align} e_i \u0026 = y_i - \\hat{y}_i = y_i - \\underbrace{\\left(b_0 + b_1 x_i\\right)}_\\text{prediction}\\\\ e_1^2 + \\dots + e_N^2 \u0026= \\sum_{i=1}^N e_i^2 \\equiv \\text{SSR}(b_0,b_1) \\\\ (b_0,b_1) \u0026= \\arg \\min_{\\text{int},\\text{slope}} \\sum_{i=1}^N \\left[y_i - \\left(\\text{int} + \\text{slope } x_i\\right)\\right]^2 \\end{align} \\]\none can derive an explicit formula for them:\n\\( \\begin{equation} b_1 = \\frac{cov(x,y)}{var(x)} \\end{equation} \\) i.e. the estimate of the slope coefficient is the covariance between $x$ and $y$ divided by the variance of $x$, both computed from our sample of data. With $b_1$ in hand, we can get the estimate for the intercept as\n\\[\\begin{equation} b_0 = \\bar{y} - b_1 \\bar{x} \\end{equation}\\]\nwhere $\\bar{z}$ denotes the sample mean of variable $z$. The interpretation of the OLS slope coefficient $b_1$ is as follows. Given a line as in $y = b_0 + b_1 x$,\n $b_1 = \\frac{d y}{d x}$ measures the change in $y$ resulting from a one unit change in $x$ For example, if $y$ is wage and $x$ is years of education, $b_1$ would measure the effect of an additional year of education on wages.  There is an alternative representation for the OLS slope coefficient which relates to the correlation coefficient $r$. Remember that $r = \\frac{cov(x,y)}{s_x s_y}$, where $s_z$ is the standard deviation of variable $z$. With this in hand, we can derive the OLS slope coefficient as\n$$ \\begin{align} b_1 \u0026amp;= \\frac{cov(x,y)}{var(x)}\\\n\u0026amp;= \\frac{cov(x,y)}{s_x s_x} \\\\ \u0026amp;= r\\frac{s_y}{s_x} \\end{align}  $$\nIn other words, the slope coefficient is equal to the correlation coefficient $r$ times the ratio of standard deviations of $y$ and $x$.\nLinear Regression without Regressor \\[ \\begin{equation} y = b_0 \\end{equation} \\]\nThis means that our minimization problem becomes very simple: We only have to choose $b_0$! We have\n\\( b_0 = \\arg\\min_{\\text{int}} \\sum_{i=1}^N \\left[y_i - \\text{int}\\right]^2, \\) which is a quadratic equation with a unique optimum such that \\( b_0 = \\frac{1}{N} \\sum_{i=1}^N y_i = \\overline{y}. \\)\n Least Squares without regressor $x$ estimates the sample mean of the outcome variable $y$, i.e. it produces $\\overline{y}$.\n Regression without an Intercept We follow the same logic here, just that we miss another bit from our initial equation and the minimisation problem now becomes: \\( \\begin{align} b_1 \u0026= \\arg\\min_{\\text{slope}} \\sum_{i=1}^N \\left[y_i - \\text{slope } x_i \\right]^2\\\\ \\mapsto b_1 \u0026= \\frac{\\frac{1}{N}\\sum_{i=1}^N x_i y_i}{\\frac{1}{N}\\sum_{i=1}^N x_i^2} = \\frac{\\bar{x} \\bar{y}}{\\overline{x^2}} \\end{align} \\)\n Least Squares without intercept (i.e. with $b_0=0$) is a line that passes through the origin.\n In this case we only get to choose the slope $b_1$ of this anchored line.1\nCentering A Regression By centering or demeaning a regression, we mean to substract from both $y$ and $x$ their respective averages to obtain $\\tilde{y}_i = y_i - \\bar{y}$ and $\\tilde{x}_i = x_i - \\bar{x}$. We then run a regression without intercept as above. That is, we use $\\tilde{x}_i,\\tilde{y}_i$ instead of $x_i,y_i$ in\n\\[ \\begin{align} b_1 \u0026= \\arg\\min_{\\text{slope}} \\sum_{i=1}^N \\left[y_i - \\text{slope } x_i \\right]^2\\\\ \\mapsto b_1 \u0026= \\frac{\\frac{1}{N}\\sum_{i=1}^N x_i y_i}{\\frac{1}{N}\\sum_{i=1}^N x_i^2} = \\frac{\\bar{x} \\bar{y}}{\\overline{x^2}} \\end{align} \\]\nto obtain our slope estimate \\(b_1\\):\n$$ \\begin{align} b1 \u0026amp;= \\frac{\\frac{1}{N}\\sum^N \\tilde{x}_i \\tilde{y}i}{\\frac{1}{N}\\sum^N \\tilde{x}_i^2}\\\n\u0026amp;= \\frac{\\frac{1}{N}\\sum_{i=1}^N (x_i - \\bar{x}) (y_i - \\bar{y})}{\\frac{1}{N}\\sum_{i=1}^N (x_i - \\bar{x})^2} \\\\ \u0026amp;= \\frac{cov(x,y)}{var(x)}  \\end{align} $$\nThis last expression is identical to the one in OLS estimate! It's the standard OLS estimate for the slope coefficient. We note the following:\n Adding a constant to a regression produces the same result as centering all variables and estimating without intercept. So, unless all variables are centered, always include an intercept in the regression.\n Standardizing A Regression Standardizing a variable $z$ means to demean as above, but in addition to divide the demeaned value by its own standard deviation. Similarly to what we did above for centering, we define transformed variables $\\breve{y}_i = \\frac{y_i-\\bar{y}}{\\sigma_y}$ and $\\breve{x}_i = \\frac{x_i-\\bar{x}}{\\sigma_x}$ where $\\sigma_z$ is the standard deviation of variable $z$. From here on, you should by now be used to what comes next! As above, we use $\\breve{x}_i,\\breve{y}_i$ instead of $x_i,y_i$:\n$$ \\begin{align} b1 \u0026amp;= \\frac{\\frac{1}{N}\\sum^N \\breve{x}_i \\breve{y}i}{\\frac{1}{N}\\sum^N \\breve{x}_i^2}\\\n\u0026amp;= \\frac{\\frac{1}{N}\\sum_{i=1}^N \\frac{x_i - \\bar{x}}{\\sigma_x} \\frac{y_i - \\bar{y}}{\\sigma_y}}{\\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{x_i - \\bar{x}}{\\sigma_x}\\right)^2} \\\\ \u0026amp;= \\frac{Cov(x,y)}{\\sigma_x \\sigma_y} \\\\ \u0026amp;= Corr(x,y)  \\end{align} $$\n After we standardize both $y$ and $x$, the slope coefficient $b_1$ in the regression without intercept is equal to the correlation coefficient.\n Predictions and Residuals Now we want to ask how our residuals $e_i$ relate to the prediction $\\hat{y_i}$. Let us first think about the average of all predictions \\(\\hat{y_i}\\), i.e. the number \\(\\frac{1}{N} \\sum_{i=1}^N \\hat{y_i}\\). Let's just take\n\\[ \\begin{equation} \\hat{y}_i = b_0 + b_1 x_i \\end{equation} \\]\nand plug this into this average, so that we get\n\\[ \\begin{align} \\frac{1}{N} \\sum_{i=1}^N \\hat{y_i} \u0026= \\frac{1}{N} \\sum_{i=1}^N b_0 + b_1 x_i \\\\ \u0026= b_0 + b_1 \\frac{1}{N} \\sum_{i=1}^N x_i \\\\ \u0026= b_0 + b_1 \\bar{x} \\\\ \\end{align} \\]\nBut that last line is just equal to the formula for the OLS intercept $b_0 = \\bar{y} - b_1 \\bar{x}$! That means of course that\n\\( \\frac{1}{N} \\sum_{i=1}^N \\hat{y_i} = b_0 + b_1 \\bar{x} = \\bar{y} \\) in other words:\n The average of our predictions $\\hat{y_i}$ is identically equal to the mean of the outcome $y$. This implies that the average of the residuals is equal to zero.\n Related to this result, we can show that the prediction $\\hat{y}$ and the residuals are uncorrelated, something that is often called orthogonality between $\\hat{y}_i$ and $e_i$. We would write this as\n\\[ \\begin{align} Cov(\\hat{y},e) \u0026=\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i-\\bar{y})(e_i-\\bar{e}) = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i-\\bar{y})e_i \\\\ \u0026= \\frac{1}{N} \\sum_{i=1}^N \\hat{y}_i e_i-\\bar{y} \\frac{1}{N} \\sum_{i=1}^N e_i = 0 \\end{align} \\]\nCorrelation, Covariance and Linearity It is important to keep in mind that Correlation and Covariance relate to a linear relationship between x and y. Given how the regression line is estimated by OLS (see just above), you can see that the regression line inherits this property from the Covariance.\n Always visually inspect your data, and don't rely exclusively on summary statistics like mean, variance, correlation and regression line. All of those assume a linear relationship between the variables in your data.\n Analysing $Var(y)$ Analysis of Variance (ANOVA) refers to a method to decompose variation in one variable as a function of several others. We can use this idea on our outcome $y$. Suppose we wanted to know the variance of $y$, keeping in mind that, by definition, $y_i = \\hat{y}_i + e_i$. We would write\n\\[ \\begin{align}Var(y) \u0026= Var(\\hat{y} + e)\\\\ \u0026= Var(\\hat{y}) + Var(e) + 2 Cov(\\hat{y},e)\\\\ \u0026= Var(\\hat{y}) + Var(e) \\end{align} \\]\nWe have seen that the covariance between prediction $\\hat{y}$ and error $e$ is zero, that's why we have $Cov(\\hat{y},e)=0$. What this tells us in words is that we can decompose the variance in the observed outcome $y$ into a part that relates to variance as explained by the model and a part that comes from unexplained variation. Finally, we know the definition of variance, and can thus write down the respective formulae for each part:\n \\[Var(y) = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\bar{y})^2\\]\n \\(Var(\\hat{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y_i} - \\bar{y})^2\\), because the mean of $\\hat{y}$ is $\\bar{y}$ as we know.\n Finally, \\(Var(e) = \\frac{1}{N}\\sum_{i=1}^N e_i^2\\), because the mean of $e$ is zero. We can thus formulate how the total variation in outcome $y$ is apportioned between model and unexplained variation:\n   The total variation in outcome $y$ (often called SST, or total sum of squares) is equal to the sum of explained squares (SSE) plus the sum of residuals (SSR). We have thus SST = SSE + SSR.\n Assessing the Goodness of Fit In our setup, there exists a convenient measure for how good a particular statistical model fits the data. It is called $R^2$ (R squared), also called the coefficient of determination. We make use of the just introduced decomposition of variance, and write the formula as\n\\[ \\begin{equation}R^2 = \\frac{\\text{variance explained}}{\\text{total variance}} = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\in[0,1] \\end{equation} \\]\nIt is easy to see that a good fit is one where the sum of explained squares (SSE) is large relative to the total variation (SST). In such a case, we observe an $R^2$ close to one. In the opposite case, we will see an $R^2$ close to zero. Notice that a small $R^2$ does not imply that the model is useless, just that it explains a small fraction of the observed variation.\n  This slope is related to the angle between vectors $\\mathbf{a} =(\\overline{x},\\overline{y})$, and $\\mathbf{b} = (\\overline{x},0)$. Hence, it's related to the scalar projection of $\\mathbf{a}$ on $\\mathbf{b}$] ^   ","date":1562198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569399923,"objectID":"36fcacd15121fd6b14fe64cb86a26064","permalink":"/post/linear-regression/","publishdate":"2019-07-04T00:00:00Z","relpermalink":"/post/linear-regression/","section":"post","summary":"OLS estimator The method to compute (or estimate) $b_0$ and $b_1$ we illustrated above is called Ordinary Least Squares, or OLS. $b_0$ and $b_1$ are therefore also often called the OLS coefficients. By solving problem\n\\[ \\begin{align} e_i \u0026 = y_i - \\hat{y}_i = y_i - \\underbrace{\\left(b_0 + b_1 x_i\\right)}_\\text{prediction}\\\\ e_1^2 + \\dots + e_N^2 \u0026= \\sum_{i=1}^N e_i^2 \\equiv \\text{SSR}(b_0,b_1) \\\\ (b_0,b_1) \u0026= \\arg \\min_{\\text{int},\\text{slope}} \\sum_{i=1}^N \\left[y_i - \\left(\\text{int} + \\text{slope } x_i\\right)\\right]^2 \\end{align} \\]","tags":["econometrics-with-r"],"title":"Linear Regression","type":"post"},{"authors":[],"categories":["R"],"content":"效應評估模型 \\[Y_{i}={Y}_{-p,i}+\\beta_i P_{i}\\]\n\\[ Y_i=Y_{-P,i}+\\beta^* P_i \\]\n\\[ \\begin{equation} Y_i=\\beta_0+\\beta_1P_i+w_i'\\gamma+\\varepsilon \\tag{3.2} \\end{equation} \\]\n在$w_{i}$條件下，「香煙售價」$P_{i}$必需要與「非價格效應的香煙銷售量」$Y_{-P}$獨立，即：\\(P_i\\perp Y_{-p,i} | w_i\\) 另一個同義說法是：「香煙售價」$P_{i}$必需要與「控制$w_{i}$條件後的非價格效應香煙銷售量」獨立。\n对$Y_{-P}$进行$rincome$下分解 \\( \\begin{equation} Y_{i}=Y_{-P,i}-\\mathbb{E}(Y_{-P,i}|rincome_{i})+\\beta^{*}P_{i}+\\mathbb{E}(Y_{-P,i}|rincome_{i}) \\tag{3.3} \\end{equation} \\)\n把資料依$w_{i}$條件變數不同, 分群觀察「香煙售價」$P_{i}$與「香煙銷售量」$Y_{i}$之間的斜率。如果$w_{i}$變數選得好，同一群資料$P_{i}$與$Y_{i}$間的關連會反映應有的效應斜率——雖然有時$Y_{i}$會因為$Y_{-P,i}$的干擾影響我們對斜率高低的觀察，但因為$Y_{-P,i}$不會與$P_{i}$有關了，這些觀察干擾在大樣本下會互相抵消掉而還原應有的效應斜率值。\n如果不管我們怎麼選擇$w_{i}$還是無法控制住$Y_{-P,i}$對與關連$Y_{i}$的干擾，那我們就要進行【資料轉換】直接從原始資料中【去除這些干擾】，其中最常見的兩種去除法為：工具變數法、追蹤資料固定效果模型。\n 工具變數法：透過工具變數留下$P_{i}$不與$Y_{-P,i}$相關的部份。 追蹤資料：透過變數轉換去除$P_{i}$中與$Y_{-P,i}$相關的部份。  \\[ Y_i=Y_{-p,i}+\\beta\\mathbb{E}(P_i|z_i)+\\beta (P_i-\\mathbb{E}(P_i|z_i)) \\]\nRelevance condition $\\mathbb{E}(P|z)\\neq 常数$即$z$对$P$具有解释力\nExclusion condition \\(Y_{-p,i}+\\beta(P_i-\\mathbb{E}(P_i|z_i))\\)与\\(z_{i}\\)无关\n三个假设 \\[ \\begin{equation} Y_i=\\beta_0+\\beta_1 P_i + \\gamma_1 rincome_i + \\epsilon_i \\tag{3.5} \\end{equation} \\]\n Q1: 我的工具變數有滿足排除條件（或外生條件）嗎?   香煙稅是否與控制條件下的「非售價因素銷售」無關？\n \\[ Y =\\underset{(\\times k)}{X}\\beta+\\underset{(\\times p)}{W}\\gamma +\\epsilon \\]\n其中$X$為要進行效應評估的變數群，$W$為控制變數群，故$ϵ$為「$W$控制條件下排除$X$效果的Y值」。另外，我們額外找了工具變數: $\\underset{\\times m)}{Z}$, 要驗證：\n$H_{0}$: 工具變數$Z$與迴歸模型誤差項$ϵ$無關\n 進行TSLS，取得 \\( \\hat{\\epsilon}_{_{TSLS}}=Y-\\hat{Y}_{TSLS} \\). 將 \\( \\hat{\\epsilon}_{_{TSLS}} \\) 迴歸在總工具變數群（即$Z$與$W$）並進行所有係數為0的聯立檢定，計算檢定量 $J=mF\\sim\\chi^{2}(m-k)$，其中F係數聯立檢定的F檢定值。   此檢定的自由度為$m−k$，所以$m$要大於$k$。“等於”時是無法進行檢定的。\n  Q2: 我的工具變數關聯性夠強嗎？   香煙稅真的與「售價」很有關連嗎？\n 工具變數$Z$必需要與效應解釋變數$X$有「足夠強」的關聯，否則\\(\\hat{\\beta}_{_{TSLS}}\\)的大樣本漸近分配不會是常態分配。\n考慮TSLS中的第一階段迴歸模型：$X=Z\\alpha_z+W\\alpha_w+u$我們希望$\\alpha_z$聯立夠顯著。\n檢定原則\n$H_0$:$Z$ 工具變數只有微弱關聯性。\n $X$迴歸在「總」工具變數群($Z$,$W$)，進行$\\alpha_z=0$的聯立F檢定。 $F\u0026gt;10$拒絕$H_0$。   Q3: 我對遺漏變數偏誤(OVB)的擔心是否多餘？  或許根本沒有必要用工具變數，在(3.5)迴歸模型下，PP早已和ϵϵ（即「控制條件下的非售價因素銷售」）無關——直接對(3.5)進行最小平方法估計即可。 \\( \\begin{equation} Y =X\\beta+W\\gamma +\\epsilon \\tag{3.6} \\end{equation} \\) $H_0 $: 迴歸模型(3.6)中的$\\beta$係數估計「沒有」面臨OVB: 用OLS或TSLS都可以: 在大樣本下，\\(\\\\hat{\\beta}_{OLS}\\approx\\hat{\\beta}_{TSLS}\\)。\n$H_1 $: 迴歸模型(3.6)中的$\\beta$係數估計「有」面臨OVB: 只能用TSLS :在大樣本下，\\(\\\\hat{\\beta}_{OLS}\\neq \\hat{\\beta}_{TSLS}\\)。\nHausman檢定統計量: \\( H\\equiv\\left(\\hat{\\beta}_{IV}-\\hat{\\beta}_{OLS}\\right)^{'}\\left[V(\\hat{\\beta}_{IV}-\\hat{\\beta}_{OLS})\\right]^{-1}\\left(\\hat{\\beta}_{IV}-\\hat{\\beta}_{OLS}\\right)\\sim\\chi_{(df)}^{2}. \\) – df： $\\beta$係數個數.\n 當$H\u0026gt;\\chi_{(df)}^{2}(\\alpha)$才拒絕$H_0$。  ","date":1562198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569399580,"objectID":"d44edb0bc44794c491b4675e582ef1cf","permalink":"/post/iv/","publishdate":"2019-07-04T00:00:00Z","relpermalink":"/post/iv/","section":"post","summary":"效應評估模型 \\[Y_{i}={Y}_{-p,i}+\\beta_i P_{i}\\] \\[ Y_i=Y_{-P,i}+\\beta^* P_i \\] \\[ \\begin{equation} Y_i=\\beta_0+\\beta_1P_i+w_i'\\gamma+\\varepsilon \\tag{3.2} \\end{equation} \\] 在$w_{i}$條件下，「香煙售價」$P_{i}$必需要與「非價格效應的香煙銷售量」$Y_{-P}$獨立","tags":["econometrics-with-r"],"title":"工具变量","type":"post"},{"authors":[],"categories":["Workflow"],"content":" Sep 25, 2019 的update: 这个WorkFlow不太完美，现在转用Blogdown和Git来管理，正在摸索中。\n总算把Ghost配得七七八八，以后要好好记下笔记了。像以前看过的东西时间久了就全忘了，太郁闷了。\n目前的Workflow如下  在Synology Drive下Draft目录存放草稿 Typora里写markdown并保存 存Leanote和evernote各一份，这个应该可以通过IFTTT来实现，日后研究 另外一个解决方案是直接Git init Draft目录，再往Github上push备份。 存Ghost发布  需要的代码注入 公式 在Post Header 粘贴以下脚本\n\u0026lt;script type=\u0026quot;text/javascript\u0026quot; src=\u0026quot;https://cdn.bootcss.com/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026quot;text/x-mathjax-config\u0026quot;\u0026gt; MathJax.Hub.Config({ tex2jax: { inlineMath: [['$$','$$'], ['\\\\\\\\(','\\\\\\\\)']], processEscapes: true } }); \u0026lt;/script\u0026gt;  语法高亮 在Post Header 粘贴以下脚本\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/prism/1.16.0/themes/prism-tomorrow.css\u0026quot;\u0026gt;  在Post Footer粘贴以下脚本\n\u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/prism/1.16.0/prism.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/prism/1.16.0/components/prism-python.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/prism/1.16.0/components/prism-r.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/prism/1.16.0/components/prism-sas.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/prism/1.16.0/components/prism-bash.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;  prism.js不支持Stata就凑合着用用吧。需要载入的components取决于博文需要。\n需要注意的地方  Ghost对于H1不能生成Toc，从H2开始 对于Markdown中的公式有些需要转义  ","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569400838,"objectID":"fd0d5c3a676ce31710c1a973ea77f6b5","permalink":"/post/ghost-blog-workflow/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/post/ghost-blog-workflow/","section":"post","summary":"Sep 25, 2019 的update: 这个WorkFlow不太完美，现在转用Blogdown和Git来管理，正在摸索中。 总算把Ghost配得七七八八，以后要","tags":["Workflow"],"title":"Ghost Blog Workflow","type":"post"},{"authors":[],"categories":["Stata"],"content":"Odds ratios An odds ratio of 1.0 is equivalent to a beta weight of 0.0.\n   Group Diseased Healthy     Exposed $D_E$ $H_E$   Not exposed $D_N$ $H_N$    $OR={\\frac {D_{E}/H_{E}}{D_{N}/H_{N}}}$\nThe distribution of the odds ratio is far from normal. Take the natural logarithm of the odds ratio to get normal.\n$logit = ln(OR)$\nWhen the mean is around 0.50, the OLS regression and logistic regression produce consistent results, but when the probability is close to 0 or 1, the logistic regression is especially important.\nLogistic regression The logit command gives the regression coefficients to estimate the logit score. The logistic command gives us the odds ratios we need to interpret the effect size of the predictors.\nBoth commands give the same results, except that logit gives the coefficients for estimating the logit score and logistic gives the odds ratios.\nThe McFadden pseudo-$R^2$ represents how much larger log likelihood is for the final solution. , meaning the log likelihood for the fitted model is 2% larger than for the log likelihood for the intercept-only model. This is not explained variance. The pseudo-$R^2$ is often a small value, and many researchers do not report it. The biggest mistake is to report it and interpret it as explained variance.\nIf you are interested in specific effects of individual variables, it is better to rely on odds ratios for interpreting results of logistic regression. This shows that mothers who smoke have 2.02 times greater odds of having a low-birthweight child.\nOdds ratios tell us what happens to the odds of an outcome, whereas risk ratios tell us what happens to their probability.\nFor binary predictor variables, you can interpret the odds ratios and percentages directly. For variables that are not binary, you need to have some other standard. One solution is to compare specific examples, such as having no dinners with the family versus having seven dinners with them each week. Another solution is to evaluate the effect of a 1-standard-deviation change for variables that are not binary.listcoef,get from package spost13. After logit/logitstic regression, run listcoef, helpor listcoef, help percent\n   Group Experimental (E) Control (C)     Events (E) EE CE   Non-events (N) EN CN    $ RR={\\frac {EE/(EE+EN)}{CE/(CE+CN)}}={\\frac {EE(CE+CN)}{CE(EE+EN)}}. $ 相对风险是指在暴露在某条件下，一个事件的发生风险 oddsrisk $OR={\\frac {EE/CE}{EN/CN}}={\\frac {EE\\cdot CN}{EN\\cdot CE}}$ 一个事件发生比是该事件发生和不发生的比率 Risk ratio is different from the odds ratio, although it asymptotically approaches it for small probabilities of outcomes. If EE is substantially smaller than EN, then EE/(EE + EN) $ \\scriptstyle \\approx $ EE/EN. Similarly, if CE is much smaller than CN, then CE/(CN + CE) $ \\scriptstyle \\approx $ CE/CN. $ RR={\\frac {EE(CE+CN)}{CE(EE+EN)}}\\approx {\\frac {EE\\cdot CN}{EN\\cdot CE}}=OR. $\nThe difference is small with a rare outcome.The relative risk is appealing, but it should not be used in a study that controls the number of people in each category.\nHypothesis testing chi-squared test that has k degrees of freedom, tells us only that the overall model has at least one significant predictor.\nTesting individual coefficients The z test in the Stata output is actually the square root of the Wald chi-squared test.\nThe likelihood-ratio chi-squared test for each parameter estimate is based on comparing two logistic models, one with the individual variable we want to test included and one without it. The likelihood-ratio test is the difference in the likelihood-ratio chi-squared values for these two models (this appears as LR chi2(1) near the upper right corner of the output). The difference between the two likelihood-ratio chi-squared values is 1 degree of freedom.\nuse nlsy97_chapter11, clear logistic drank30 male dinner97 pdrink97 estimates store a logistic drank30 age97 male dinner97 pdrink97 #subtracts the chi-squared values and estimates the probability of the chi-squared difference; lrtest a  or just use lrdrop1\nTesting sets of coefficients test pdrink97 dinner97 #it is the same as: logistic drank30 age97 male if !mi(dinner97) \u0026amp;!mi(pdrink97) estimates store a logistic drank30 age97 male pdrink97 dinner97 lrtest a lrdrop1  this overall test only tells us that at least one of them is significant.\nMargins logit drank30 age97 i.black pdrink97 dinner97 margins, dydx(black) atmeans margins black, atmeans margins, at(pdrink97=(1 2 3 4 5)) atmeans marginsplot  We can run the logistic regression using the i. label for this categorical variable, i.black. This produces the same results for the logistic regression as if we had simply used black, but the results will work properly if we follow this command with other postestimation commands.\nNested logistic regressions The nestreg command is extremely general, applicable across a variety of regression models, including logistic, negative binomial, Poisson, probit, ordered logistic, tobit, and others. It also works with the complex sample designs for many regression models.\nPower analysis powerlog, p1(.70) p2(.75) alpha(.05) powerlog, p1(.70) p2(.75) alpha(.05) rsq(.30) help  ","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569402239,"objectID":"a878f12385f10d9d428fd83f4819ebba","permalink":"/post/logistic-regression/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/post/logistic-regression/","section":"post","summary":"Odds ratios An odds ratio of 1.0 is equivalent to a beta weight of 0.0. Group Diseased Healthy Exposed $D_E$ $H_E$ Not exposed $D_N$ $H_N$ $OR={\\frac {D_{E}/H_{E}}{D_{N}/H_{N}}}$ The distribution of the odds ratio is far from normal. Take the natural logarithm of the odds ratio to get normal. $logit = ln(OR)$ When the mean is around 0.50, the OLS regression and logistic regression produce consistent results, but when the probability","tags":["Stata"],"title":"Logistic Regression","type":"post"},{"authors":[],"categories":["Stata"],"content":"Constructing a Scale recode empathy2 empathy4 empathy5 (1=5 \u0026quot;Does not describe very well\u0026quot;) /// (2=4) (3=3) (4=2) (5=1 \u0026quot;Describes very well\u0026quot;), pre(rev) label(empathy) egen empathy = rowmean(empathy1 revempathy2 empathy3 revempathy4 /// revempathy5 empathy6 empathy7) egen miss = rowmiss(empathy1 revempathy2 empathy3 revempathy4 /// revempathy5 empathy6 empathy7) egen empathya = rowmean(empathy1 revempathy2 empathy3 revempathy4 /// revempathy5 empathy6 empathy7) if miss \u0026lt; 3  One drawback to using the rowmean() function is that it simply adds the score on the items a person answers and divides by the number of items answered.\nReliability  Stability means that if you measure a variable today using a particular scale and then measure it again tomorrow using the same scale, your results will be consistent.(correlation r,pwcorr, intraclass correlation $\\rho_I$) Equivalence means that you have two measures of the same variable and they produce consistent results. (correlation $r_{xx}$)* (A low correlation means either that the measure is not reliable or that the measures are not truly equivalent.) A reliable test would be internally consistent if the score for the first half of the items was highly correlated with the score for the second half of the items.(correlation \\(r_{x_Ax_B}\\)), alpha,\\(\\alpha\\)) In general, an $\\alpha\u0026gt;0.8$ is considered good reliability, and many researchers feel an $\\alpha\u0026gt;0.7$ is adequate reliability. (\\(\\alpha=\\sigma^2_{True}/(\\sigma^2_{True}+\\sigma^2_{error})\\))However, for this interpretation to be used, we need to assume that the scale is valid.  alpha empathy1 revempathy2 empathy3 revempathy4 revempathy5 /// empathy6 empathy7, asis item min(5) The asis (as is) option means that we do not want Stata to change the signs of any of our variables. The bottom row of the output table, Test scale, reports the $\\alpha$ for the scale (0.7462). Above this value is the $\\alpha$ we would obtain if we dropped each item, one at a time. The item-test correlation column reports the correlation of each item with the total score of the seven items. item-rest correlation. This is the correlation of each item with the total of the other items. The equivalent of alpha for items that are dichotomous is the Kuder–Richardson measure of reliability.alpha Rater consistency is important when you have observers rating a video, observed behavior, essay, or something else where two or more people are rating the same information. Here reliability means that a pair of raters gives consistent results.(kappa,$\\kappa$ kap coder1 coder2)$\\kappa$ only gives us credit for the extent the agreement exceeds what we would have expected to get by chance alone. kappa tends to be lower than alpha.\nValidity A valid measure is one that measures what it is supposed to be measuring.\n 表面效度(face validity)：把設計的問卷，拿給親朋好友填，並問他們問卷好不好。指測量工具在外顯形式上的有效程度\n 內容效度(content validity)：找一群有相關經驗的人來看題目，問他們設計的好不好，有沒有哪裡要修改。Content validity ratio (CVR): Judges rate each item as essential, useful, or not necessary. $CVR=(Ne - N/2)/(N/2)$ , in which the $Ne$ is the number of panelists indicating \u0026quot;essential\u0026quot; and $N$ is the total number of panelists. You can keep the items that have a relatively high CVR and drop those that do not.\n 效標效度(criterion validity)：把測量工具和其他可測量的工具，算他們之間的相關n以測驗分數和特定效標（criterion）之間的相關係數，表示測量工具有效性之高低。\n （1）同時效度(current validity)：把設計好的題目，和標準工具（同樣的觀念，相同的變項），去算之間的相關。如：測疼痛忍受度，有四題一分鐘可測完的題目，和另一份標準工具的題目，45題1小時可做完的題目去測，如果R＝0.92（高相關），表示原題目有同時效度。 （2）預測效度(predictive validity)：一個調查，可以預測未來的事件、行為、態度、結果。如：手術後，病人對止痛藥的需求，看24個病人的分數，分數越高，手術忍受度越高。把24的分數算出，和拿止痛藥量求相關，R＝－0.82，表示高忍痛程度，低止痛藥量。SAT（可以預測大學第一學期的平均成績）成績，和大學第一學期的平均成績求相關，R＝0.42，表示沒有預測效度。但是R如果逐年增加，則表示有預測效度。  構念（建構）效度(construct validity)：\n We can assess the convergent and divergent validity of our measure, hope, by seeing whether it is positively correlated with variables with which we believe it converges and negatively correlated with variables with which we believe it diverges.ttest, esize, pwcorr\nFactor analysis  exploratory factor analysis, which Stata calls principal factor analysis: the variance is partitioned into the shared variance and unique or error variance. The shared variance is how much of the variance in any one item can be explained by the rest of the items. PF\n principal-component factor analysis PCF\n  putdocx stata 15可以create word documents!\nTerminology  Extraction(萃取) Eigenvalues: In the case of PCF analysis, If there are 10 items, the sum of the eigenvalues will be 10.The factors will be ordered from the most important, which has the largest eigenvalue, to the least important, which has the smallest eigenvalue.In PF analysis, the sum of the eigenvalues will be less than the number of items, and the eigenvalues’ interpretation is complex. Communality and uniqueness: PF analysis tries to explain the shared variance. PCF analysis tries to explain all the variance, which is why it is ideal for the uniqueness to approach zero. Loadings: how clusters of items are most related to one or another of the factors. If an item has a loading over 0.4 on a factor, it is considered a good indicator of that factor. Simple structure: This is a pattern of loadings where each item loads strongly on just one factor and a subset of items load strongly on each factor. When an item loads strongly on more than one factor, it is factorially confounded. Scree plot: This is a graph showing the eigenvalue for each factor. When doing a PCF analysis, we usually drop factors that have eigenvalues in the neighborhood of 1.0 or smaller. Rotation: 轉軸的方式有很多種，但基本就是兩大類：正交 (orthogonal) 與斜交 (oblique rotation)。轉軸的目的是讓因素更有意義，並同時看看因素之間的關係。更詳細一點來說，如果是正交轉軸的話，那就是假設因素之間沒有關連；相對地，斜交假設因素之間有一定的關連。 Factor score: weights each item based on how related it is to the factor. Also the factor score is scaled to have a mean of 0.0 and a variance of 1.0.  Use PCF when you have a set of items that you believe all measure one concept. In this situation, you would be interested in the first principal factor. You would want to see if it explained a substantial part of the total variance for the entire set of items, and you would want most of the items to have a loading of 0.4 or above on this factor. Because PCF analysis is trying to explain all the variance in the items, the uniqueness for each item should approach zero. Generally, we should consider any factor that has an eigenvalue of more than 1.A visual way to examine the eigenvalues is with a scree plot.\nfactor rnatspac rnatenvir rnatheal rnatcity rnatcrime rnatdrug /// rnateduc rnatrace rnatarms rnatfare rnatroad rnatsoc rnatchld rnatsci, pcf screeplot  If, on the other hand, you want to identify two or more latent variables that represent interpretable dimensions of some concept, then PF analysis is probably best.\nRotation  Orthogonal:rotateWith a varimax rotation, we can think of the loadings as being the estimated correlation between each item and each factor. oblique:rotate, promax  estat common to get correlation matrix of promax rotated common factors\nGet one factor score However, this distinction rarely makes a lot of practical difference. The factor score may make a difference if there are some items with very large loadings, say, 0.9, and others with very small loadings, say, 0.2. But we would probably drop the weakest items. When the loadings do not vary a great deal, computing a factor score or a mean/total score will produce comparable results.\nfactor rnatenvir rnatheal rnatcity rnatcrime rnatdrug rnateduc rnatrace /// rnatfare rnatsoc rnatchld, pcf predict libfscore, norotate egen libmean = rowmean(rnatenvir rnatheal rnatcity rnatcrime rnatdrug /// rnateduc rnatrace rnatfare rnatsoc rnatchld)  correlation higher than 0.9...\n","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569401484,"objectID":"26d964d256f033b533e051c923e27d46","permalink":"/post/measurement-reliability-and-validity/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/post/measurement-reliability-and-validity/","section":"post","summary":"Constructing a Scale recode empathy2 empathy4 empathy5 (1=5 \u0026quot;Does not describe very well\u0026quot;) /// (2=4) (3=3) (4=2) (5=1 \u0026quot;Describes very well\u0026quot;), pre(rev) label(empathy) egen empathy = rowmean(empathy1 revempathy2 empathy3 revempathy4 /// revempathy5 empathy6 empathy7) egen miss = rowmiss(empathy1 revempathy2 empathy3 revempathy4 /// revempathy5 empathy6 empathy7) egen empathya = rowmean(empathy1 revempathy2 empathy3 revempathy4 /// revempathy5 empathy6 empathy7) if miss \u0026lt; 3 One drawback to using the rowmean() function is that it","tags":["Stata"],"title":"Measurement, reliability, and validity","type":"post"},{"authors":[],"categories":["Stata"],"content":" Many advanced Stata estimation models can use multiple imputation for handling missing values.\nAuxiliary variables are variables that can help to make estimates on incomplete data, while they are not part of the main analysis (Collins et al., 2001).\n Include all variables in the analysis model, including the dependent variable, Include auxiliary variables that predict patterns of missingness, and Include additional variables that predict a person’s score on a variable that has missing values.  The imputation model is then used to generate a complete dataset.\nOnce you have included a reasonably large number of variables, adding additional variables may not be helpful because of multicollinearity.\nDrop any participant who does not have complete information on every item used in the analysis. This approach goes by several names, including full case analysis, casewise deletion, or listwise deletion.\n There will be a substantial loss of power because of the reduced sample size. Listwise deletion can introduce substantial bias. (survival bias)  One alternative to listwise deletion involves substituting the mean on a variable for anybody who does not have a response. This has two serious limitations. People who are average on a variable are often more likely to give an answer than are people who have an extreme value.The second problem with mean substitution is that when you give several people the same score on a variable, these people have zero variance on the variable. This artificially reduced variance will seriously bias our parameter estimates.\nThe key to understanding multiple imputation is that the imputed missing values will not contain any unique information once the variables in the model and the auxiliary variables are allowed to explain the patterns of missing values and predict the score of the missing values. The imputed values for variables with missing values are simply consistent with the observed data. This allows us to use all available information in our analysis.\nMultiple imputation A powerful way of working with missing values involves multiple imputation. The command mi involves three straightforward steps:\n Create m complete datasets by imputing the missing values. Each dataset will have no missing values, but the values imputed for missing values will vary across the datasets. Do your analysis in each of the m complete datasets. Pool your m solutions to get one solution.  The parameter estimates—for example, regression coefficients—will be the mean of their corresponding values in the datasets. The standard errors used for testing significance will combine the standard errors from the solutions plus the variance of the parameter estimates across the solutions. If each solution is yielding a very different estimate, this uncertainty is added to the standard errors. Also the degrees of freedom is adjusted based on the number of imputations and proportion of data that have missing values.   The most widely used approach is using multivariate normal regression (MVN). mi impute mvn is designed for continuous variables. mi impute chained is another useful alternative.\nA missing value will have a code of ., .a, .b, etc. Remember that a missing value is recorded in a Stata dataset as an extremely high value. Within mi, a missing-value code, . (dot), has a special meaning. It denotes the missing values eligible for imputation. If you have a set of missing values that should not be imputed, you should record them as extended missing values, that is, as .a, .b, etc.recode agem (.a = .)\nmisstable summarize ln_wagem gradem agem ttl_expm tenurem not_smsa south blackm misstable patterns ln_wagem gradem agem ttl_expm tenurem not_smsa south blackm quietly misstable summarize ln_wagem gradem agem ttl_expm tenurem not_smsa south blackm, gen(miss_)  then\nlogit miss_ln_wagem gradem agem ttl_expm tenurem not_smsa south blackm if ln_wagem \u0026lt;= . logit miss_gradem ln_wagem agem ttl_expm tenurem not_smsa south blackm if gradem \u0026lt;= . logit miss_agem ln_wagem gradem ttl_expm tenurem not_smsa south blackm if agem \u0026lt;= . logit miss_ttl_expm ln_wagem gradem agem tenurem not_smsa south blackm if ttl_expm \u0026lt;= . logit miss_tenurem ln_wagem gradem agem ttl_expm not_smsa south blackm if tenurem \u0026lt;= . logit miss_blackm ln_wagem gradem agem ttl_expm tenurem not_smsa south if blackm \u0026lt;= .  Or use pwcorr , obs sig to find potential auxiliary variables.\nAny variable that is statistically significant in these logistic regressions should be included in the imputation step.\nmi set flong mi register imputed ln_wagem gradem agem ttl_expm tenurem blackm mi register regular not_smsa south  The mi set flong command tells Stata how to arrange our multiple datasets(flong (full and long), or mlong (marginal and long)). The mi register imputed command registers all the variables that have missing values and need to be imputed. The mi register regular command registers all the variables that have no missing values or for which we do not want to impute values.\nmi impute mvn ln_wagem gradem agem ttl_expm tenurem blackm, add(20) rseed(2121)  生成m=20个数据集，_mi_m variable identifies datasets and ranges from 0 to 20.\nmi impute mvn ln_wagem gradem agem ttl_expm tenurem blackm, add(20) rseed(2121)  To get pooled $R^2$ and standardized $\\beta$s use mibeta\nmibeta ln_wagem gradem agem ttl_expm tenurem not_smsa south blackm, fisherz miopts(vartable)  When impossible values are imputed(建议不调整): Binary variables, squares, and interactions（在原数据集先相乘，再impute）\n","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569401322,"objectID":"80878b249b8067a93b348727d3e2cff9","permalink":"/post/missing-values/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/post/missing-values/","section":"post","summary":"Many advanced Stata estimation models can use multiple imputation for handling missing values. Auxiliary variables are variables that can help to make estimates on incomplete data, while they are not part of the main analysis (Collins et al., 2001). Include all variables in the analysis model, including the dependent variable, Include auxiliary variables that predict patterns of missingness, and Include additional variables that predict a perso","tags":["Stata"],"title":"Missing values","type":"post"},{"authors":[],"categories":["Stata"],"content":"Multilevel analysis can address the lack of independence of the observations when you are analyzing grouped data. See Stata Multilevel Mixed-Effects Reference Manual.\n groups of individuals panel data  Fixed-effects regression models \\[y_it = \\beta_0 +\\beta x_{it}+\\mu_i+\\eta_{it}\\]\nif \\(\\mu_i\\) correlates with \\(x_{it}\\) -\u0026gt; Fixed-effects if \\(\\mu_i\\) independent of \\(x_{it}\\) -\u0026gt; Random-effects models give consistent estimates\nxtreg see Stata Longitudinal-Data/Panel-Data Reference Manual.\nRandom-effects regression models \\[y_it = \\beta_0 +\\beta x_{it}+\\gamma z_i +\\mu_i+\\eta_{it}\\]\nassume \\(\\mu_i\\) is independent of \\(x_{it}\\)\nfixed component, \\( \\beta_0 +\\beta x_{it}+\\gamma z_i\\) , describes the overall relationship between our dependent variable and our independent variable. The random component, \\(\\mu_i\\) i represents the effects of the unobserved time-invariant variables.\nscore = fixed part + random effects + error\nGoing back and forth between wide and long formats : reshape wide and reshape long\nreshape long drink, i(id) j(wave)  Random-intercept model linear model mixed drink c.wave || id: estimates store linear margins, at(wave=(0(2)10)) marginsplot  quadratic term mixed drink c.wave##c.wave || id: estimates store quadratic margins, at(wave=(0(2)10)) marginsplot lrtest linear quadratic  A proportional reduction in error (PRE) measuring how much the residual (error) variance is reduced by adding the quadratic term may be useful. We will call the random-intercept linear model “Model 1” and the random-intercept quadratic model “Model 2”.\nPRE = (var(Residual)Model1-var(Residual)Model2)/var(Residual)Model1\nTreating time as a categorical variable mixed drink i.wave || id: estimates store means margins, at(wave=(0(2)10)) marginsplot lrtest linear means lrtest quadratic means  Random-coefficients model mixed drink c.wave || id: wave, cov(unstructured) predict yhat_drink, fitted  Including a time-invariant covariate * Random coefficients model with time invariant covariate * gender coded as male = 1, female = 0 mixed drink c.wave i.male || id: wave margins male, at(wave=(0(2)8)) marginsplot * Random coefficients, with wave interacting with the * time invariant covariate--gender coded mixed drink c.wave##i.male || id: wave margins male, at(wave=(0(2)8)) marginsplot mixed drink c.wave##c.wave##i.male || id: wave margins male, at(wave=(0(2)8)) marginsplot  ","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569401094,"objectID":"74b42c3a300de18f788b325aa3113f68","permalink":"/post/multilevel-analysis/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/post/multilevel-analysis/","section":"post","summary":"Multilevel analysis can address the lack of independence of the observations when you are analyzing grouped data. See Stata Multilevel Mixed-Effects Reference Manual.\n groups of individuals panel data  Fixed-effects regression models \\[y_it = \\beta_0 +\\beta x_{it}+\\mu_i+\\eta_{it}\\]\nif \\(\\mu_i\\) correlates with \\(x_{it}\\) -\u0026gt; Fixed-effects if \\(\\mu_i\\) independent of \\(x_{it}\\) -\u0026gt; Random-effects models give consistent estimates\nxtreg see Stata Longitudinal-Data/Panel-Data Reference Manual.\nRandom-effects regression models \\[y_it = \\beta_0 +\\beta x_{it}+\\gamma z_i +\\mu_i+\\eta_{it}\\]","tags":["Stata"],"title":"Multilevel analysis","type":"post"},{"authors":[],"categories":["Stata"],"content":" Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nNote: toc is not compatible with markup: mmark\nBasic  F: There is a highly significant relationship between outcomes and the set of predictors. R2: How much of the outcome variance is explained by the regression model Adj-R2: remove the chance effects Coef.: unstandardized regression coefficients t: coef/standard error Std. Err.: represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable. ,beta gives beta weights: based on standardizing all variables to have a mean of 0 and a standard deviation of 1. These beta weights are interpreted similarly to how you interpret correlations in that betamulticollinearity problem):a 1-standard-deviation change in the independent variable produces a - beta standard-deviation change in the dependent variable. increment in R2:part-correlation square because it measures the part that is uniquely explained by the variable. or semipartial R2 (Semipartial Corr.^2 in pcorr )estimates only the unique effect of each predictor. Another way to compare is partial correlation; distribution of the dependent variable: histogram env_con, frequency normal kdensity (for kernel density estimation)Skewness(0:Normal; 0: positive or skew to the right)kurtosis(3: normal; 3: tails are too thin, peaky or positive kurtosis)sktest distribution of the residuals: for large sample, normality is not a critical issue. rvfplot, yline(0)residual-versus-fitted plot: To solve the non-normal distribution of residual, we can use reg y xs, vce(robust) or use bootstrapreg y xs, vce(bootstrap, rep(1000)) , it will change std err and hence t-value. However, Andrew J. Leone, Miguel Minutti-Meza, and Charles E. Wasley (2019) Influential Observations and Inference in Accounting Research. The Accounting Review In-Press. they talk about robust regression using robreg, what\u0026rsquo;s the difference? ALso, check Correcting for Cross-Sectional and Time-Series Dependence in Accounting Research\nregress env_con educat inc com3 hlthprob epht3, beta predict envhat preserve set seed 515 sample 100, count twoway (scatter env_con envhat) (lfit env_con envhat) restore   Diagnostic statistics  Rstandard:  The standardized residual is the residual divided by its standard deviation.\nregress env_con educat inc com3 hlthprob epht3, beta predict yhat predict residual, residual predict rstandard, rstandard list respnum env_con yhat residual rstandard if abs(rstandard) \u0026gt; 2.58 \u0026amp; rstandard \u0026lt; . dfbeta list respnum rstandard _dfbeta_1 if abs(_dfbeta_1) \u0026gt; 2/sqrt(3769) \u0026amp; _dfbeta_1 \u0026lt; . estat vif   Influential observations: DFbeta: You could think of this as redoing the regression model, omitting just one observation at a time and seeing how much difference omitting each observation makes. ****A value of **DFbeta \u0026gt;2/sqrt(N) ** indicates that an observation has a large influence**** More specific than rstandard\n. dfbeta (739 missing values generated) _dfbeta_1: dfbeta(educat) (739 missing values generated) _dfbeta_2: dfbeta(inc) (739 missing values generated) _dfbeta_3: dfbeta(com3) (739 missing values generated) _dfbeta_4: dfbeta(hlthprob) (739 missing values generated) _dfbeta_5: dfbeta(epht3)  multicollinearity: The more correlated the predictors, the more they overlap and, hence, the more difficult it is to identify their independent effects. In such situations, you can have multicollinearity in which one or more of the predictors are virtually redundant. variance inflation factor estat vif after regression, if \u0026gt;10, for any variable, a multicollinearity problem may exist. If the average VIF is substantially greater than 1.00, there still could be a problem.(Dropping a variable, create a scale that combines them into one variable.) 1/VIF = 1-R2(of regress X1 on other Xs) It tells how much of the variance in the independent variable is available to predict the outcome variable independently.\nWeighted data regress env_con educat inc com3 hlthprob epht3 [pweight=finalwt], beta  When you do a weighted regression this way, Stata automatically uses the robust regression—whether you ask for it or not—because weighted data require robust standard errors.\nCategorical predictors and hierarchical regression regress smday97 age97 male psmoke97 aa hispanic other if !missing(smday97, /// age97, male, psmoke97, aa, hispanic, other), beta test aa hispanic other  nested regressions\nnestreg: regress smday97 (age97 male) (psmoke97) (aa hispanic other), beta  If you put i. as a stub in front of a categorical variable, Stata will make the first category the reference category and then generate a dummy variable for each of the remaining categories.\nregress smday97 age97 male psmoke97 i.race #change reference category or what Stata refers to as the baselevel regress smday97 age97 male psmoke97 ib3.race regress smday97 age97 male psmoke97 ib(last).race  interaction g ed_male = educ*male reg inc educ male ed_male,beta nestreg: regress inc (educ male) (ed_male), beta regress inc i.male##c.educ, beta  some researchers choose to center quantitative independent variables, such as education, before computing the interaction terms. Centering is important for independent variables where a value of zero may not be meaningful.\nsummarize educ generate educ_c = educ - r(mean)  margins help us to interpret the interaction term\nmargins male, at(educ=(8 10 12 14 16 18)) marginsplot   nonlinear regress ln_wage c.ttl_exp##c.ttl_exp, beta margins, at(ttl_exp = (0(2)28)) marginsplot  Power analysis\n","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569401650,"objectID":"654821b264d710452607d6f517c55993","permalink":"/post/multiple-regressions/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/post/multiple-regressions/","section":"post","summary":"Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nNote: toc is not compatible with markup: mmark\nBasic  F: There is a highly significant relationship between outcomes and the set of predictors. R2: How much of the outcome variance is explained by the regression model Adj-R2: remove the chance effects Coef.: unstandardized regression coefficients t: coef/standard error Std. Err.: represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable.","tags":["Stata"],"title":"Multiple Regressions","type":"post"},{"authors":[],"categories":["Books"],"content":"Bosons make up one of the two classes of particles, the other being fermions.\nSo far, we have some hints and some ideas about what the smallest distance in the universe might be (the Planck length). We have a pretty good catalog of twelve matter particles that so far we haven’t been able to break further apart (the Standard Model). And we have a list of three possible ways that these particles can interact (the electroweak and strong forces and gravity).\n","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569401961,"objectID":"63db4c98a6735ed7c4a7e48ff11197cd","permalink":"/post/we-have-no-idea/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/post/we-have-no-idea/","section":"post","summary":"Bosons make up one of the two classes of particles, the other being fermions.\nSo far, we have some hints and some ideas about what the smallest distance in the universe might be (the Planck length). We have a pretty good catalog of twelve matter particles that so far we haven’t been able to break further apart (the Standard Model). And we have a list of three possible ways that these particles can interact (the electroweak and strong forces and gravity).","tags":[],"title":"We have no idea","type":"post"},{"authors":["Yihong WANG"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Yihong WANG","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Yihong WANG","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]