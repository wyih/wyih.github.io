<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Yihong WANG">

  
  
  
    
  
  <meta name="description" content="OLS estimator The method to compute (or estimate) $b_0$ and $b_1$ we illustrated above is called Ordinary Least Squares, or OLS. $b_0$ and $b_1$ are therefore also often called the OLS coefficients. By solving problem
\[ \begin{align} e_i & = y_i - \hat{y}_i = y_i - \underbrace{\left(b_0 &#43; b_1 x_i\right)}_\text{prediction}\\ e_1^2 &#43; \dots &#43; e_N^2 &= \sum_{i=1}^N e_i^2 \equiv \text{SSR}(b_0,b_1) \\ (b_0,b_1) &= \arg \min_{\text{int},\text{slope}} \sum_{i=1}^N \left[y_i - \left(\text{int} &#43; \text{slope } x_i\right)\right]^2 \end{align} \]">

  
  <link rel="alternate" hreflang="en-us" href="/post/linear-regression/">

  


  
  
  
  <meta name="theme-color" content="#4caf50">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.4a2804b4d4c4cd4583bb17ba271d35ae.css">

  

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/linear-regression/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Little World">
  <meta property="og:url" content="/post/linear-regression/">
  <meta property="og:title" content="Linear Regression | Little World">
  <meta property="og:description" content="OLS estimator The method to compute (or estimate) $b_0$ and $b_1$ we illustrated above is called Ordinary Least Squares, or OLS. $b_0$ and $b_1$ are therefore also often called the OLS coefficients. By solving problem
\[ \begin{align} e_i & = y_i - \hat{y}_i = y_i - \underbrace{\left(b_0 &#43; b_1 x_i\right)}_\text{prediction}\\ e_1^2 &#43; \dots &#43; e_N^2 &= \sum_{i=1}^N e_i^2 \equiv \text{SSR}(b_0,b_1) \\ (b_0,b_1) &= \arg \min_{\text{int},\text{slope}} \sum_{i=1}^N \left[y_i - \left(\text{int} &#43; \text{slope } x_i\right)\right]^2 \end{align} \]"><meta property="og:image" content="/img/icon-192.png">
  <meta property="twitter:image" content="/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-07-04T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-09-25T16:25:23&#43;08:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/linear-regression/"
  },
  "headline": "Linear Regression",
  
  "datePublished": "2019-07-04T00:00:00Z",
  "dateModified": "2019-09-25T16:25:23+08:00",
  
  "author": {
    "@type": "Person",
    "name": "Yihong WANG"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Little World",
    "logo": {
      "@type": "ImageObject",
      "url": "/img/icon-512.png"
    }
  },
  "description": "OLS estimator The method to compute (or estimate) $b_0$ and $b_1$ we illustrated above is called Ordinary Least Squares, or OLS. $b_0$ and $b_1$ are therefore also often called the OLS coefficients. By solving problem\n\\[ \\begin{align} e_i \u0026 = y_i - \\hat{y}_i = y_i - \\underbrace{\\left(b_0 + b_1 x_i\\right)}_\\text{prediction}\\\\ e_1^2 + \\dots + e_N^2 \u0026= \\sum_{i=1}^N e_i^2 \\equiv \\text{SSR}(b_0,b_1) \\\\ (b_0,b_1) \u0026= \\arg \\min_{\\text{int},\\text{slope}} \\sum_{i=1}^N \\left[y_i - \\left(\\text{int} + \\text{slope } x_i\\right)\\right]^2 \\end{align} \\]"
}
</script>

  

  


  


  





  <title>Linear Regression | Little World</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Little World</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Linear Regression</h1>

  

  
    



<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Sep 25, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/r/">R</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/linear-regression/&amp;text=Linear%20Regression" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/linear-regression/&amp;t=Linear%20Regression" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Linear%20Regression&amp;body=/post/linear-regression/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/linear-regression/&amp;title=Linear%20Regression" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Linear%20Regression%20/post/linear-regression/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/linear-regression/&amp;title=Linear%20Regression" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="ols-estimator">OLS estimator</h2>

<p>The method to compute (or <em>estimate</em>) $b_0$ and $b_1$ we illustrated above is called <em>Ordinary Least Squares</em>, or OLS. $b_0$ and $b_1$ are therefore also often called the <em>OLS coefficients</em>. By solving problem</p>

<p><span  class="math">\[
\begin{align}
e_i & = y_i - \hat{y}_i = y_i - \underbrace{\left(b_0 + b_1 x_i\right)}_\text{prediction}\\
e_1^2 + \dots + e_N^2 &= \sum_{i=1}^N e_i^2 \equiv \text{SSR}(b_0,b_1) \\
(b_0,b_1) &= \arg \min_{\text{int},\text{slope}} \sum_{i=1}^N \left[y_i - \left(\text{int} + \text{slope } x_i\right)\right]^2 
\end{align}
\]</span></p>

<p>one can derive an explicit formula for them:</p>

<p><span  class="math">\(
\begin{equation}
b_1 = \frac{cov(x,y)}{var(x)}
\end{equation}
\)</span>
i.e. the estimate of the slope coefficient is the covariance between $x$ and $y$ divided by the variance of $x$, both computed from our sample of data. With $b_1$ in hand, we can get the estimate for the intercept as</p>

<p><span  class="math">\[\begin{equation}
b_0 = \bar{y} - b_1 \bar{x}
\end{equation}\]</span></p>

<p>where $\bar{z}$ denotes the sample mean of variable $z$. The interpretation of the OLS slope coefficient $b_1$ is as follows. Given a line as in $y = b_0 + b_1 x$,</p>

<ul>
<li>$b_1 = \frac{d y}{d x}$ measures the change in $y$ resulting from a one unit change in $x$</li>
<li>For example, if $y$ is wage and $x$ is years of education, $b_1$ would measure the effect of an additional year of education on wages.</li>
</ul>

<p>There is an alternative representation for the OLS slope coefficient which relates to the <em>correlation coefficient</em> $r$. Remember that $r = \frac{cov(x,y)}{s_x s_y}$, where $s_z$ is the standard deviation of variable $z$. With this in hand, we can derive the OLS slope coefficient as</p>

<p>$$
\begin{align}
b_1 &amp;= \frac{cov(x,y)}{var(x)}\</p>

<pre><code>&amp;= \frac{cov(x,y)}{s_x s_x} \\
&amp;= r\frac{s_y}{s_x} \end{align}
</code></pre>

<p>$$</p>

<p>In other words, the slope coefficient is equal to the correlation coefficient $r$ times the ratio of standard deviations of $y$ and $x$.</p>

<h3 id="linear-regression-without-regressor">Linear Regression without Regressor</h3>

<p><span  class="math">\[
\begin{equation}
y = b_0
\end{equation}
\]</span></p>

<p>This means that our minimization problem becomes very simple: We only have to choose $b_0$! We have</p>

<p><span  class="math">\(
b_0 = \arg\min_{\text{int}} \sum_{i=1}^N \left[y_i - \text{int}\right]^2,
\)</span>
which is a quadratic equation with a unique optimum such that
<span  class="math">\(
b_0 = \frac{1}{N} \sum_{i=1}^N y_i = \overline{y}.
\)</span></p>

<blockquote>
<p>Least Squares <strong>without regressor</strong> $x$ estimates the sample mean of the outcome variable $y$, i.e. it produces $\overline{y}$.</p>
</blockquote>

<h3 id="regression-without-an-intercept">Regression without an Intercept</h3>

<p>We follow the same logic here, just that we miss another bit from our initial equation and the minimisation problem now becomes:
<span  class="math">\(
\begin{align}
b_1 &= \arg\min_{\text{slope}} \sum_{i=1}^N \left[y_i - \text{slope } x_i \right]^2\\
\mapsto b_1 &= \frac{\frac{1}{N}\sum_{i=1}^N x_i y_i}{\frac{1}{N}\sum_{i=1}^N x_i^2} = \frac{\bar{x} \bar{y}}{\overline{x^2}} 
\end{align}
\)</span></p>

<blockquote>
<p>Least Squares <strong>without intercept</strong> (i.e. with $b_0=0$) is a line that passes through the origin.</p>
</blockquote>

<p>In this case we only get to choose the slope $b_1$ of this anchored line.<sup class="footnote-ref" id="fnref:fn1"><a class="footnote" href="#fn:fn1">1</a></sup></p>

<h3 id="centering-a-regression">Centering A Regression</h3>

<p>By <em>centering</em> or <em>demeaning</em> a regression, we mean to substract from both $y$ and $x$ their respective averages to obtain $\tilde{y}_i = y_i - \bar{y}$ and $\tilde{x}_i = x_i - \bar{x}$. We then run a regression <em>without intercept</em> as above. That is, we use $\tilde{x}_i,\tilde{y}_i$ instead of $x_i,y_i$ in</p>

<p><span  class="math">\[
\begin{align}
b_1 &= \arg\min_{\text{slope}} \sum_{i=1}^N \left[y_i - \text{slope } x_i \right]^2\\
\mapsto b_1 &= \frac{\frac{1}{N}\sum_{i=1}^N x_i y_i}{\frac{1}{N}\sum_{i=1}^N x_i^2} = \frac{\bar{x} \bar{y}}{\overline{x^2}} 
\end{align}
\]</span></p>

<p>to obtain our slope estimate <span  class="math">\(b_1\)</span>:</p>

<p>$$
\begin{align}
b<em>1 &amp;= \frac{\frac{1}{N}\sum</em>^N \tilde{x}_i \tilde{y}<em>i}{\frac{1}{N}\sum</em>^N \tilde{x}_i^2}\</p>

<pre><code>&amp;= \frac{\frac{1}{N}\sum_{i=1}^N (x_i - \bar{x}) (y_i - \bar{y})}{\frac{1}{N}\sum_{i=1}^N (x_i - \bar{x})^2} \\
&amp;= \frac{cov(x,y)}{var(x)}
</code></pre>

<p>\end{align}
$$</p>

<p>This last expression is <em>identical</em> to the one in OLS estimate! It's the standard OLS estimate for the slope coefficient. We note the following:</p>

<blockquote>
<p>Adding a constant to a regression produces the same result as centering all variables and estimating without intercept. So, unless all variables are centered, <strong>always</strong> include an intercept in the regression.</p>
</blockquote>

<h3 id="reg-standard">Standardizing A Regression</h3>

<p><em>Standardizing</em> a variable $z$ means to demean as above, but in addition to divide the demeaned value by its own standard deviation. Similarly to what we did above for <em>centering</em>, we define transformed variables $\breve{y}_i = \frac{y_i-\bar{y}}{\sigma_y}$ and $\breve{x}_i = \frac{x_i-\bar{x}}{\sigma_x}$ where $\sigma_z$ is the standard deviation of variable $z$. From here on, you should by now be used to what comes next! As above, we use $\breve{x}_i,\breve{y}_i$ instead of $x_i,y_i$:</p>

<p>$$
\begin{align}
b<em>1 &amp;= \frac{\frac{1}{N}\sum</em>^N \breve{x}_i \breve{y}<em>i}{\frac{1}{N}\sum</em>^N \breve{x}_i^2}\</p>

<pre><code>&amp;= \frac{\frac{1}{N}\sum_{i=1}^N \frac{x_i - \bar{x}}{\sigma_x} \frac{y_i - \bar{y}}{\sigma_y}}{\frac{1}{N}\sum_{i=1}^N \left(\frac{x_i - \bar{x}}{\sigma_x}\right)^2} \\
&amp;= \frac{Cov(x,y)}{\sigma_x \sigma_y} \\
&amp;= Corr(x,y)  
</code></pre>

<p>\end{align}
$$</p>

<blockquote>
<p>After we standardize both $y$ and $x$, the slope coefficient $b_1$ in the regression without intercept is equal to the <strong>correlation coefficient</strong>.</p>
</blockquote>

<h2 id="pred-resids">Predictions and Residuals</h2>

<p>Now we want to ask how our residuals $e_i$ relate to the prediction $\hat{y_i}$. Let us first think about the average of all predictions <span  class="math">\(\hat{y_i}\)</span>, i.e. the number <span  class="math">\(\frac{1}{N} \sum_{i=1}^N \hat{y_i}\)</span>. Let's just take</p>

<p><span  class="math">\[
\begin{equation}
\hat{y}_i = b_0 + b_1 x_i 
\end{equation}
\]</span></p>

<p>and plug this into this average, so that we get</p>

<p><span  class="math">\[
\begin{align}
\frac{1}{N} \sum_{i=1}^N \hat{y_i} &= \frac{1}{N} \sum_{i=1}^N b_0 + b_1 x_i \\
&= b_0 + b_1  \frac{1}{N} \sum_{i=1}^N x_i \\
&= b_0 + b_1  \bar{x} \\
\end{align}
\]</span></p>

<p>But that last line is just equal to the formula for the OLS intercept  $b_0 = \bar{y} - b_1 \bar{x}$! That means of course that</p>

<p><span  class="math">\(
\frac{1}{N} \sum_{i=1}^N \hat{y_i}  = b_0 + b_1  \bar{x} = \bar{y}
\)</span>
in other words:</p>

<blockquote>
<p>The average of our predictions $\hat{y_i}$ is identically equal to the mean of the outcome $y$. This implies that the average of the residuals is equal to zero.</p>
</blockquote>

<p>Related to this result, we can show that the prediction $\hat{y}$ and the residuals are <em>uncorrelated</em>, something that is often called <strong>orthogonality</strong> between $\hat{y}_i$ and $e_i$. We would write this as</p>

<p><span  class="math">\[
\begin{align}
Cov(\hat{y},e) &=\frac{1}{N} \sum_{i=1}^N (\hat{y}_i-\bar{y})(e_i-\bar{e}) =   \frac{1}{N} \sum_{i=1}^N (\hat{y}_i-\bar{y})e_i \\
&=  \frac{1}{N} \sum_{i=1}^N \hat{y}_i e_i-\bar{y} \frac{1}{N} \sum_{i=1}^N e_i = 0
\end{align}
\]</span></p>

<h2 id="correlation-covariance-and-linearity">Correlation, Covariance and Linearity</h2>

<p>It is important to keep in mind that Correlation and Covariance relate to a <em>linear</em> relationship between <code>x</code> and <code>y</code>. Given how the regression line is estimated by OLS (see just above), you can see that the regression line inherits this property from the Covariance.</p>

<blockquote>
<p>Always <strong>visually inspect</strong> your data, and don't rely exclusively on summary statistics like <em>mean, variance, correlation and regression line</em>. All of those assume a <strong>linear</strong> relationship between the variables in your data.</p>
</blockquote>

<h2 id="analysing-vary">Analysing $Var(y)$</h2>

<p>Analysis of Variance (ANOVA) refers to a method to decompose variation in one variable as a function of several others. We can use this idea on our outcome $y$. Suppose we wanted to know the variance of $y$, keeping in mind that, by definition, $y_i = \hat{y}_i + e_i$. We would write</p>

<p><span  class="math">\[
\begin{align}Var(y) &= Var(\hat{y} + e)\\ &= Var(\hat{y}) + Var(e) + 2 Cov(\hat{y},e)\\ &= Var(\hat{y}) + Var(e) \end{align}
\]</span></p>

<p>We have seen that the covariance between prediction $\hat{y}$ and error $e$ is zero, that's why we have $Cov(\hat{y},e)=0$. What this tells us in words is that we can decompose the variance in the observed outcome $y$ into a part that relates to variance as <em>explained by the model</em> and a part that comes from unexplained variation. Finally, we know the definition of <em>variance</em>, and can thus write down the respective formulae for each part:</p>

<ul>
<li><p><span  class="math">\[Var(y) = \frac{1}{N}\sum_{i=1}^N (y_i - \bar{y})^2\]</span></p></li>

<li><p><span  class="math">\(Var(\hat{y}) = \frac{1}{N}\sum_{i=1}^N (\hat{y_i} - \bar{y})^2\)</span>, because the mean of $\hat{y}$ is $\bar{y}$ as we know.</p></li>

<li><p>Finally, <span  class="math">\(Var(e) = \frac{1}{N}\sum_{i=1}^N e_i^2\)</span>, because the mean of $e$ is zero.
We can thus formulate how the total variation in outcome $y$ is apportioned between model and unexplained variation:</p></li>
</ul>

<blockquote>
<p>The total variation in outcome $y$ (often called SST, or <em>total sum of squares</em>) is equal to the sum of explained squares (SSE) plus the sum of residuals (SSR). We have thus <strong>SST = SSE + SSR</strong>.</p>
</blockquote>

<h2 id="assessing-the-goodness-of-fit">Assessing the <em>Goodness of Fit</em></h2>

<p>In our setup, there exists a convenient measure for how good a particular statistical model fits the data. It is called $R^2$ (<em>R squared</em>), also called the <em>coefficient of determination</em>. We make use of the just introduced decomposition of variance, and write the formula as</p>

<p><span  class="math">\[
\begin{equation}R^2 = \frac{\text{variance explained}}{\text{total variance}} = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\in[0,1]  \end{equation}
\]</span></p>

<p>It is easy to see that a <em>good fit</em> is one where the sum of <em>explained</em> squares (SSE) is large relative to the total variation (SST). In such a case, we observe an $R^2$ close to one. In the opposite case, we will see an $R^2$ close to zero. Notice that a small $R^2$ does not imply that the model is useless, just that it explains a small fraction of the observed variation.</p>
<div class="footnotes">

<hr>

<ol>
<li id="fn:fn1">This slope is related to the angle between vectors $\mathbf{a} =(\overline{x},\overline{y})$, and $\mathbf{b} = (\overline{x},0)$. Hence, it's related to the <a href="https://en.wikipedia.org/wiki/Scalar_projection">scalar projection</a> of $\mathbf{a}$ on $\mathbf{b}$]
 <a class="footnote-return" href="#fnref:fn1"><sup>^</sup></a></li>
</ol>
</div>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/econometrics-with-r/">econometrics-with-r</a>
  
</div>



    
      








  






  
  
  
    
  
  
  <div class="media author-card">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu52a603635ecebd45650b162dadabb4e5_12861_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Yihong WANG</a></h5>
      
      <p class="card-text">Wayfaring Stranger</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/wyih" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/iv/">工具变量</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.0bf1e3db85cbb232372ed31d6f10dc70.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
   
  <p class="powered-by">
    &copy;Yihong WANG 2019 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
